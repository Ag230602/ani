{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this as:\n",
    "# C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\scripts\\extract_hurdat2_irma_ian.py\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "BASE = r\"C:\\Users\\Adrija\\Downloads\\DFGCN\\data\"\n",
    "HURDAT_PATH = os.path.join(BASE, \"raw\", \"hurdat2\", \"hurdat2_atlantic.txt\")\n",
    "OUT_DIR = os.path.join(BASE, \"processed\", \"tracks\")\n",
    "\n",
    "# Irma 2017 = AL112017, Ian 2022 = AL092022\n",
    "TARGET_IDS = {\"AL112017\": \"irma_2017\", \"AL092022\": \"ian_2022\"}\n",
    "\n",
    "def parse_lat_lon(lat_str: str, lon_str: str):\n",
    "    # examples:  \"25.1N\", \"80.3W\"\n",
    "    lat = float(lat_str[:-1])\n",
    "    if lat_str.endswith(\"S\"):\n",
    "        lat = -lat\n",
    "    lon = float(lon_str[:-1])\n",
    "    if lon_str.endswith(\"W\"):\n",
    "        lon = -lon\n",
    "    return lat, lon\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    current_id = None\n",
    "    current_name = None\n",
    "    rows_by_id = {sid: [] for sid in TARGET_IDS}\n",
    "\n",
    "    with open(HURDAT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Header line: AL112017, IRMA, 51\n",
    "            if line.startswith(\"AL\") and \",\" in line:\n",
    "                parts = [p.strip() for p in line.split(\",\")]\n",
    "                current_id = parts[0]\n",
    "                current_name = parts[1] if len(parts) > 1 else None\n",
    "                continue\n",
    "\n",
    "            if current_id in TARGET_IDS:\n",
    "                parts = [p.strip() for p in line.split(\",\")]\n",
    "                # date, time, record_id, status, lat, lon, vmax, mslp, ...\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "\n",
    "                date = parts[0]\n",
    "                time = parts[1]\n",
    "                status = parts[3]\n",
    "                lat_str = parts[4]\n",
    "                lon_str = parts[5]\n",
    "                vmax = parts[6]\n",
    "                mslp = parts[7]\n",
    "\n",
    "                # keep synoptic times only (6-hourly)\n",
    "                if time not in {\"0000\", \"0600\", \"1200\", \"1800\"}:\n",
    "                    continue\n",
    "\n",
    "                lat, lon = parse_lat_lon(lat_str, lon_str)\n",
    "\n",
    "                rows_by_id[current_id].append({\n",
    "                    \"storm_id\": current_id,\n",
    "                    \"storm_name\": current_name,\n",
    "                    \"date\": date,\n",
    "                    \"time\": time,\n",
    "                    \"datetime_utc\": f\"{date[:4]}-{date[4:6]}-{date[6:8]} {time[:2]}:{time[2:]}:00\",\n",
    "                    \"status\": status,\n",
    "                    \"lat\": lat,\n",
    "                    \"lon\": lon,\n",
    "                    \"vmax_kt\": int(vmax) if vmax.isdigit() else \"\",\n",
    "                    \"mslp_mb\": int(mslp) if mslp.isdigit() else \"\",\n",
    "                })\n",
    "\n",
    "    for sid, tag in TARGET_IDS.items():\n",
    "        out_path = os.path.join(OUT_DIR, f\"{tag}_hurdat2.csv\")\n",
    "        with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as wf:\n",
    "            writer = csv.DictWriter(\n",
    "                wf,\n",
    "                fieldnames=[\n",
    "                    \"storm_id\",\"storm_name\",\"date\",\"time\",\"datetime_utc\",\n",
    "                    \"status\",\"lat\",\"lon\",\"vmax_kt\",\"mslp_mb\"\n",
    "                ],\n",
    "            )\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows_by_id[sid])\n",
    "\n",
    "        print(f\"Wrote {len(rows_by_id[sid])} rows -> {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0ae083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDS client OK\n"
     ]
    }
   ],
   "source": [
    "import cdsapi\n",
    "c = cdsapi.Client()\n",
    "print(\"CDS client OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb0a4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cdsapi in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.7.7)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: ecmwf-datastores-client>=0.4.0 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdsapi) (0.4.1)\n",
      "Requirement already satisfied: requests>=2.5.0 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdsapi) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdsapi) (4.67.1)\n",
      "Requirement already satisfied: attrs in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ecmwf-datastores-client>=0.4.0->cdsapi) (24.3.0)\n",
      "Requirement already satisfied: multiurl>=0.3.7 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ecmwf-datastores-client>=0.4.0->cdsapi) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ecmwf-datastores-client>=0.4.0->cdsapi) (4.12.2)\n",
      "Requirement already satisfied: pytz in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from multiurl>=0.3.7->ecmwf-datastores-client>=0.4.0->cdsapi) (2024.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\adrija\\appdata\\roaming\\python\\python312\\site-packages (from multiurl>=0.3.7->ecmwf-datastores-client>=0.4.0->cdsapi) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.5.0->cdsapi) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.5.0->cdsapi) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.5.0->cdsapi) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.5.0->cdsapi) (2024.12.14)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adrija\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil->multiurl>=0.3.7->ecmwf-datastores-client>=0.4.0->cdsapi) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\adrija\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->cdsapi) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 08:03:28,837 INFO Request ID is 33e5de44-5a2d-4361-9a0f-336d220de802\n",
      "2026-01-07 08:03:29,028 INFO status has been updated to accepted\n",
      "2026-01-07 08:03:51,173 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb81f636a2f41cda53e5dd1a06cf3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ba80c89c5aaa0f0f616e96413613522e.nc:   0%|          | 0.00/6.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\era5\\irma_2017\\era5_pl_irma_2017.nc\n"
     ]
    }
   ],
   "source": [
    "# Save this as:\n",
    "# C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\scripts\\download_era5_irma_2017.py\n",
    "\n",
    "%pip install cdsapi\n",
    "\n",
    "import os\n",
    "import cdsapi\n",
    "\n",
    "BASE = r\"C:\\Users\\Adrija\\Downloads\\DFGCN\\data\"\n",
    "OUTDIR = os.path.join(BASE, \"raw\", \"era5\", \"irma_2017\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "c = cdsapi.Client()\n",
    "\n",
    "# Florida + Gulf + Bahamas coverage, storage-safe:\n",
    "AREA = [35, -90, 20, -70]  # North, West, South, East\n",
    "\n",
    "c.retrieve(\n",
    "    \"reanalysis-era5-pressure-levels\",\n",
    "    {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"format\": \"netcdf\",\n",
    "        \"variable\": [\"u_component_of_wind\", \"v_component_of_wind\", \"geopotential\"],\n",
    "        \"pressure_level\": [\"850\", \"500\"],\n",
    "        \"year\": \"2017\",\n",
    "        \"month\": [\"08\", \"09\"],\n",
    "        \"day\": [\n",
    "            \"30\",\"31\",  # Aug\n",
    "            \"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"  # Sep\n",
    "        ],\n",
    "        \"time\": [\"00:00\",\"06:00\",\"12:00\",\"18:00\"],\n",
    "        \"area\": AREA,\n",
    "    },\n",
    "    os.path.join(OUTDIR, \"era5_pl_irma_2017.nc\"),\n",
    ")\n",
    "\n",
    "print(\"Saved:\", os.path.join(OUTDIR, \"era5_pl_irma_2017.nc\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0042317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 08:04:52,976 INFO Request ID is 7a5c3e1e-bb80-4de0-b7f1-db97bddac432\n",
      "2026-01-07 08:04:53,207 INFO status has been updated to accepted\n",
      "2026-01-07 08:05:07,541 INFO status has been updated to running\n",
      "2026-01-07 08:06:10,017 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5e6ce09c1d4b5f88c182f38d460d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "7fdc13d545a35258fa10af1fd127337d.nc:   0%|          | 0.00/5.92M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\era5\\ian_2022\\era5_pl_ian_2022.nc\n"
     ]
    }
   ],
   "source": [
    "# Save this as:\n",
    "# C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\scripts\\download_era5_ian_2022.py\n",
    "\n",
    "import os\n",
    "import cdsapi\n",
    "\n",
    "BASE = r\"C:\\Users\\Adrija\\Downloads\\DFGCN\\data\"\n",
    "OUTDIR = os.path.join(BASE, \"raw\", \"era5\", \"ian_2022\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "c = cdsapi.Client()\n",
    "\n",
    "AREA = [35, -90, 20, -70]  # North, West, South, East\n",
    "\n",
    "c.retrieve(\n",
    "    \"reanalysis-era5-pressure-levels\",\n",
    "    {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"format\": \"netcdf\",\n",
    "        \"variable\": [\"u_component_of_wind\", \"v_component_of_wind\", \"geopotential\"],\n",
    "        \"pressure_level\": [\"850\", \"500\"],\n",
    "        \"year\": \"2022\",\n",
    "        \"month\": [\"09\", \"10\"],\n",
    "        \"day\": [\n",
    "            \"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",  # Sep\n",
    "            \"01\",\"02\",\"03\"  # Oct\n",
    "        ],\n",
    "        \"time\": [\"00:00\",\"06:00\",\"12:00\",\"18:00\"],\n",
    "        \"area\": AREA,\n",
    "    },\n",
    "    os.path.join(OUTDIR, \"era5_pl_ian_2022.nc\"),\n",
    ")\n",
    "\n",
    "print(\"Saved:\", os.path.join(OUTDIR, \"era5_pl_ian_2022.nc\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c152291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\processed\\tracks\\irma_2017_hurdat2.csv rows= 59\n",
      "Wrote: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\processed\\tracks\\ian_2022_hurdat2.csv rows= 35\n"
     ]
    }
   ],
   "source": [
    "import os, csv\n",
    "import pandas as pd\n",
    "\n",
    "BASE = r\"C:\\Users\\Adrija\\Downloads\\DFGCN\\data\"\n",
    "HURDAT = os.path.join(BASE, \"raw\", \"hurdat2\", \"hurdat2_atlantic.txt\")\n",
    "OUTDIR = os.path.join(BASE, \"processed\", \"tracks\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "TARGET = {\"AL112017\": \"irma_2017\", \"AL092022\": \"ian_2022\"}\n",
    "\n",
    "def parse_lat_lon(lat_str, lon_str):\n",
    "    lat = float(lat_str[:-1]) * (-1 if lat_str.endswith(\"S\") else 1)\n",
    "    lon = float(lon_str[:-1]) * (-1 if lon_str.endswith(\"W\") else 1)\n",
    "    return lat, lon\n",
    "\n",
    "rows = {sid: [] for sid in TARGET}\n",
    "\n",
    "current_id = None\n",
    "current_name = None\n",
    "\n",
    "with open(HURDAT, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"AL\") and \",\" in line:\n",
    "            p = [x.strip() for x in line.split(\",\")]\n",
    "            current_id = p[0]\n",
    "            current_name = p[1] if len(p) > 1 else \"\"\n",
    "            continue\n",
    "\n",
    "        if current_id in TARGET:\n",
    "            p = [x.strip() for x in line.split(\",\")]\n",
    "            if len(p) < 8:\n",
    "                continue\n",
    "            date, time = p[0], p[1]\n",
    "            status = p[3]\n",
    "            lat_str, lon_str = p[4], p[5]\n",
    "            vmax, mslp = p[6], p[7]\n",
    "\n",
    "            # keep only 6-hourly\n",
    "            if time not in {\"0000\", \"0600\", \"1200\", \"1800\"}:\n",
    "                continue\n",
    "\n",
    "            lat, lon = parse_lat_lon(lat_str, lon_str)\n",
    "            rows[current_id].append({\n",
    "                \"storm_id\": current_id,\n",
    "                \"storm_name\": current_name,\n",
    "                \"datetime_utc\": f\"{date[:4]}-{date[4:6]}-{date[6:8]} {time[:2]}:{time[2:]}:00\",\n",
    "                \"status\": status,\n",
    "                \"lat\": lat,\n",
    "                \"lon\": lon,\n",
    "                \"vmax_kt\": int(vmax) if vmax.isdigit() else None,\n",
    "                \"mslp_mb\": int(mslp) if mslp.isdigit() else None\n",
    "            })\n",
    "\n",
    "for sid, tag in TARGET.items():\n",
    "    df = pd.DataFrame(rows[sid])\n",
    "    df[\"datetime_utc\"] = pd.to_datetime(df[\"datetime_utc\"], utc=True)\n",
    "    df = df.sort_values(\"datetime_utc\").reset_index(drop=True)\n",
    "    out = os.path.join(OUTDIR, f\"{tag}_hurdat2.csv\")\n",
    "    df.to_csv(out, index=False)\n",
    "    print(\"Wrote:\", out, \"rows=\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d04e515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xarray\n",
      "  Using cached xarray-2025.12.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting netCDF4\n",
      "  Using cached netcdf4-1.7.4-cp311-abi3-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.26 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xarray) (2.2.0)\n",
      "Requirement already satisfied: packaging>=24.1 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xarray) (24.2)\n",
      "Requirement already satisfied: pandas>=2.2 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xarray) (2.2.3)\n",
      "Collecting cftime (from netCDF4)\n",
      "  Downloading cftime-1.6.5-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from netCDF4) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\adrija\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.2->xarray) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.2->xarray) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.2->xarray) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adrija\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=2.2->xarray) (1.17.0)\n",
      "Using cached xarray-2025.12.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached netcdf4-1.7.4-cp311-abi3-win_amd64.whl (21.3 MB)\n",
      "Downloading cftime-1.6.5-cp312-cp312-win_amd64.whl (464 kB)\n",
      "Installing collected packages: cftime, netCDF4, xarray\n",
      "\n",
      "   ------------- -------------------------- 1/3 [netCDF4]\n",
      "   ------------- -------------------------- 1/3 [netCDF4]\n",
      "   ------------- -------------------------- 1/3 [netCDF4]\n",
      "   ------------- -------------------------- 1/3 [netCDF4]\n",
      "   ------------- -------------------------- 1/3 [netCDF4]\n",
      "   ------------- -------------------------- 1/3 [netCDF4]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   -------------------------- ------------- 2/3 [xarray]\n",
      "   ---------------------------------------- 3/3 [xarray]\n",
      "\n",
      "Successfully installed cftime-1.6.5 netCDF4-1.7.4 xarray-2025.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts nc3tonc4.exe, nc4tonc3.exe and ncinfo.exe are installed in 'c:\\Users\\Adrija\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install xarray netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e18663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " IRMA C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\processed\\samples\\irma_samples.npz\n",
      "X: (21, 5, 33, 33)  (N,F,G,G) expected\n",
      "past: (21, 4, 2)  (N,H,2)\n",
      "meta: (21, 2)  (N,2)\n",
      "y: (21, 4, 2)  (N,L,2)\n",
      "t0: (21,)\n",
      "\n",
      " IAN C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\processed\\samples\\ian_samples.npz\n",
      "X: (23, 5, 33, 33)  (N,F,G,G) expected\n",
      "past: (23, 4, 2)  (N,H,2)\n",
      "meta: (23, 2)  (N,2)\n",
      "y: (23, 4, 2)  (N,L,2)\n",
      "t0: (23,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "BASE = r\"C:\\Users\\Adrija\\Downloads\\DFGCN\\data\"\n",
    "for storm in [\"irma\",\"ian\"]:\n",
    "    path = os.path.join(BASE, \"processed\", \"samples\", f\"{storm}_samples.npz\")\n",
    "    d = np.load(path, allow_pickle=True)\n",
    "    print(\"\\n\", storm.upper(), path)\n",
    "    print(\"X:\", d[\"X\"].shape, \" (N,F,G,G) expected\")\n",
    "    print(\"past:\", d[\"past\"].shape, \" (N,H,2)\")\n",
    "    print(\"meta:\", d[\"meta\"].shape, \" (N,2)\")\n",
    "    print(\"y:\", d[\"y\"].shape, \" (N,L,2)\")\n",
    "    print(\"t0:\", d[\"t0\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75a01642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building irma:  85%|████████▌ | 47/55 [00:00<00:00, 105.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\processed\\samples\\irma_samples.npz | N=21 | skipped=26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building ian:  74%|███████▍  | 23/31 [00:00<00:00, 86.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\processed\\samples\\ian_samples.npz | N=23 | skipped=0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Save as:\n",
    "# C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\scripts\\02_build_samples_npz.py\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE = r\"C:\\Users\\Adrija\\Downloads\\DFGCN\\data\"\n",
    "\n",
    "TRACKS = {\n",
    "    \"irma\": os.path.join(BASE, \"processed\", \"tracks\", \"irma_2017_hurdat2.csv\"),\n",
    "    \"ian\":  os.path.join(BASE, \"processed\", \"tracks\", \"ian_2022_hurdat2.csv\"),\n",
    "}\n",
    "ERA5 = {\n",
    "    \"irma\": os.path.join(BASE, \"raw\", \"era5\", \"irma_2017\", \"era5_pl_irma_2017.nc\"),\n",
    "    \"ian\":  os.path.join(BASE, \"raw\", \"era5\", \"ian_2022\", \"era5_pl_ian_2022.nc\"),\n",
    "}\n",
    "\n",
    "OUTDIR = os.path.join(BASE, \"processed\", \"samples\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "GRID = 33\n",
    "CROP_DEG = 8.0\n",
    "HIST = 4\n",
    "LEADS_H = [6, 12, 24, 48]\n",
    "LEAD_STEPS = [h // 6 for h in LEADS_H]\n",
    "\n",
    "\n",
    "def open_era5(path: str) -> xr.Dataset:\n",
    "    ds = xr.open_dataset(path)\n",
    "\n",
    "    # variable names\n",
    "    u = \"u\" if \"u\" in ds.variables else \"u_component_of_wind\"\n",
    "    v = \"v\" if \"v\" in ds.variables else \"v_component_of_wind\"\n",
    "    z = \"z\" if \"z\" in ds.variables else \"geopotential\"\n",
    "    if u not in ds.variables or v not in ds.variables or z not in ds.variables:\n",
    "        raise ValueError(f\"ERA5 variables missing. Found vars: {list(ds.variables)}\")\n",
    "\n",
    "    # pressure coordinate\n",
    "    if \"level\" in ds.coords:\n",
    "        plev = \"level\"\n",
    "    elif \"pressure_level\" in ds.coords:\n",
    "        plev = \"pressure_level\"\n",
    "    else:\n",
    "        raise ValueError(f\"No pressure-level coord found. coords={list(ds.coords)}\")\n",
    "\n",
    "    # time coordinate\n",
    "    if \"time\" in ds.coords:\n",
    "        tcoord = \"time\"\n",
    "    elif \"valid_time\" in ds.coords:\n",
    "        tcoord = \"valid_time\"\n",
    "    else:\n",
    "        raise ValueError(f\"No time coord found. coords={list(ds.coords)}\")\n",
    "\n",
    "    # --- CRITICAL FIX: normalize longitudes to [-180, 180] and sort ---\n",
    "    if \"longitude\" not in ds.coords:\n",
    "        raise ValueError(\"ERA5 missing longitude coordinate\")\n",
    "\n",
    "    lon = ds[\"longitude\"]\n",
    "    # If 0..360 -> convert to -180..180\n",
    "    if float(lon.max()) > 180:\n",
    "        lon_new = ((lon + 180) % 360) - 180\n",
    "        ds = ds.assign_coords(longitude=lon_new)\n",
    "    # Sort longitudes increasing\n",
    "    ds = ds.sortby(\"longitude\")\n",
    "\n",
    "    # Latitudes sometimes descending; keep as-is but slicing will handle\n",
    "    ds.attrs[\"_u\"] = u\n",
    "    ds.attrs[\"_v\"] = v\n",
    "    ds.attrs[\"_z\"] = z\n",
    "    ds.attrs[\"_plev\"] = plev\n",
    "    ds.attrs[\"_tcoord\"] = tcoord\n",
    "    return ds\n",
    "\n",
    "\n",
    "def crop_X(ds: xr.Dataset, tstamp: pd.Timestamp, lat0: float, lon0: float) -> np.ndarray:\n",
    "    tcoord = ds.attrs[\"_tcoord\"]\n",
    "    u = ds.attrs[\"_u\"]\n",
    "    v = ds.attrs[\"_v\"]\n",
    "    z = ds.attrs[\"_z\"]\n",
    "    plev = ds.attrs[\"_plev\"]\n",
    "\n",
    "    # nearest time selection\n",
    "    dsel = ds.sel({tcoord: np.datetime64(tstamp.to_datetime64())}, method=\"nearest\")\n",
    "\n",
    "    lat_min, lat_max = lat0 - CROP_DEG, lat0 + CROP_DEG\n",
    "    lon_min, lon_max = lon0 - CROP_DEG, lon0 + CROP_DEG\n",
    "\n",
    "    # slice latitude regardless of ascending/descending\n",
    "    lat_vals = dsel[\"latitude\"].values\n",
    "    if lat_vals[0] > lat_vals[-1]:  # descending\n",
    "        lat_slice = slice(lat_max, lat_min)\n",
    "    else:\n",
    "        lat_slice = slice(lat_min, lat_max)\n",
    "\n",
    "    # longitude now guaranteed to be [-180,180] and sorted ascending\n",
    "    lon_slice = slice(lon_min, lon_max)\n",
    "\n",
    "    box = dsel.sel(latitude=lat_slice, longitude=lon_slice)\n",
    "\n",
    "    # Guard: empty crop (can happen if bounds are outside dataset)\n",
    "    if box.sizes.get(\"longitude\", 0) == 0 or box.sizes.get(\"latitude\", 0) == 0:\n",
    "        raise RuntimeError(\n",
    "            f\"Empty crop: lat[{lat_min:.2f},{lat_max:.2f}] lon[{lon_min:.2f},{lon_max:.2f}] \"\n",
    "            f\"-> got lat={box.sizes.get('latitude',0)} lon={box.sizes.get('longitude',0)}\"\n",
    "        )\n",
    "\n",
    "    def pl(varname: str, level: int) -> np.ndarray:\n",
    "        return box[varname].sel({plev: level}).values.astype(np.float32)\n",
    "\n",
    "    u850 = pl(u, 850); v850 = pl(v, 850)\n",
    "    u500 = pl(u, 500); v500 = pl(v, 500)\n",
    "    z500 = pl(z, 500)\n",
    "\n",
    "    X = np.stack([u850, v850, u500, v500, z500], axis=0)  # (F,H,W)\n",
    "\n",
    "    Xt = torch.from_numpy(X).unsqueeze(0)  # (1,F,H,W)\n",
    "    Xt = F.interpolate(Xt, size=(GRID, GRID), mode=\"bilinear\", align_corners=False)\n",
    "    return Xt.squeeze(0).numpy()  # (F,GRID,GRID)\n",
    "\n",
    "\n",
    "def build(storm: str):\n",
    "    df = pd.read_csv(TRACKS[storm])\n",
    "    df[\"datetime_utc\"] = pd.to_datetime(df[\"datetime_utc\"], utc=True)\n",
    "    df = df.sort_values(\"datetime_utc\").reset_index(drop=True)\n",
    "\n",
    "    # numeric meta\n",
    "    df[\"vmax_kt\"] = pd.to_numeric(df.get(\"vmax_kt\"), errors=\"coerce\")\n",
    "    df[\"mslp_mb\"] = pd.to_numeric(df.get(\"mslp_mb\"), errors=\"coerce\")\n",
    "\n",
    "    ds = open_era5(ERA5[storm])\n",
    "\n",
    "    X_list, past_list, meta_list, y_list, t0_list = [], [], [], [], []\n",
    "    skipped = 0\n",
    "\n",
    "    for i in tqdm(range(HIST, len(df)), desc=f\"building {storm}\"):\n",
    "        if i + max(LEAD_STEPS) >= len(df):\n",
    "            break\n",
    "\n",
    "        lat0, lon0 = float(df.loc[i, \"lat\"]), float(df.loc[i, \"lon\"])\n",
    "        t0 = df.loc[i, \"datetime_utc\"]\n",
    "\n",
    "        # past positions\n",
    "        past = []\n",
    "        for k in range(HIST, 0, -1):\n",
    "            past.append([float(df.loc[i-k, \"lat\"]), float(df.loc[i-k, \"lon\"])])\n",
    "        past = np.array(past, dtype=np.float32)\n",
    "\n",
    "        # metadata\n",
    "        vmax = df.loc[i, \"vmax_kt\"]\n",
    "        mslp = df.loc[i, \"mslp_mb\"]\n",
    "        vmax = float(vmax) if pd.notna(vmax) else 0.0\n",
    "        mslp = float(mslp) if pd.notna(mslp) else 0.0\n",
    "        meta = np.array([vmax, mslp], dtype=np.float32)\n",
    "\n",
    "        # unified X\n",
    "        try:\n",
    "            X = crop_X(ds, t0, lat0, lon0)\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # targets\n",
    "        y = []\n",
    "        for s in LEAD_STEPS:\n",
    "            y.append([float(df.loc[i+s, \"lat\"]), float(df.loc[i+s, \"lon\"])])\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "\n",
    "        X_list.append(X)\n",
    "        past_list.append(past)\n",
    "        meta_list.append(meta)\n",
    "        y_list.append(y)\n",
    "        t0_list.append(str(t0))\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        raise RuntimeError(f\"No samples built for {storm}. Check ERA5 coverage/time range.\")\n",
    "\n",
    "    out = os.path.join(OUTDIR, f\"{storm}_samples.npz\")\n",
    "    np.savez_compressed(\n",
    "        out,\n",
    "        X=np.stack(X_list),          # (N,F,GRID,GRID)\n",
    "        past=np.stack(past_list),    # (N,HIST,2)\n",
    "        meta=np.stack(meta_list),    # (N,2)\n",
    "        y=np.stack(y_list),          # (N,L,2)\n",
    "        t0=np.array(t0_list)\n",
    "    )\n",
    "    print(f\"Saved: {out} | N={len(X_list)} | skipped={skipped}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build(\"irma\")\n",
    "    build(\"ian\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
