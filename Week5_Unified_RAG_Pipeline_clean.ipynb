{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a5e6eb",
   "metadata": {
    "id": "e6a5e6eb"
   },
   "source": [
    "# Week 5 — Unified RAG Pipeline (Tracks A+B+C)\n",
    "\n",
    "**One notebook** to build and evaluate a RAG system with reranking, multimodal retrieval, and guardrails.\n",
    "\n",
    "Artifacts will be generated under `./week5_rag_pipeline/`:\n",
    "- `env_rag_adv.json`\n",
    "- `rag_adv_run_config.json`\n",
    "- `eval_queries.jsonl`\n",
    "- `ablation_results.csv`\n",
    "- `README.txt`\n",
    "- Folders: `project_materials/`, `project_images/`\n",
    "\n",
    "> In Colab, uncomment the `pip install` lines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b156e2",
   "metadata": {
    "id": "72b156e2"
   },
   "source": [
    "## 0. Setup (Installs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3562f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd3562f4",
    "outputId": "5edd6b98-dd70-41a5-ac01-89c0e379e061"
   },
   "outputs": [],
   "source": [
    " !pip install rank_bm25 sentence-transformers faiss-cpu pillow transformers timm accelerate langchain==0.2.14 # numpy pandas matplotlib tiktoken regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75a0df",
   "metadata": {
    "id": "1f75a0df"
   },
   "source": [
    "## 1. Environment & Subordinate Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b7a7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed7b7a7b",
    "outputId": "16c1a84a-17f9-4213-b654-ad0cdc608179"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, json, sys, platform, pkgutil, datetime, psutil, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== BASE PATH in Google Drive =====\n",
    "BASE = Path(\"/content/drive/MyDrive/week5_rag_pipeline\")\n",
    "BASE.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# folders for materials\n",
    "(DATA_TXT := BASE/\"project_materials\").mkdir(exist_ok=True)\n",
    "(DATA_IMG := BASE/\"project_images\").mkdir(exist_ok=True)\n",
    "\n",
    "# 1) env snapshot -> env_rag_adv.json\n",
    "env = {\n",
    "    \"created_at\": datetime.datetime.now().isoformat(),\n",
    "    \"python_version\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"installed_packages\": sorted([m.name for m in pkgutil.iter_modules()])\n",
    "}\n",
    "(env_path := BASE/\"env_rag_adv.json\").write_text(json.dumps(env, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# 2) pipeline config -> rag_adv_run_config.json\n",
    "run_cfg = {\n",
    "  \"embedding_model_text\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "  \"embedding_model_image\": \"openai/clip-vit-base-patch32\",\n",
    "  \"bm25\": {\"k1\": 1.5, \"b\": 0.75},\n",
    "  \"retriever_k\": 4,\n",
    "  \"rrf\": {\"k\": 60},\n",
    "  \"reranker\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "  \"mmr\": {\"lambda\": 0.7, \"fetch_k\": 20, \"mmr_k\": 5},\n",
    "  \"compression\": {\"method\": \"textrank\", \"target_chars\": 800},\n",
    "  \"guardrails\": {\n",
    "      \"enforce_citations\": True,\n",
    "      \"pii_redaction\": True,\n",
    "      \"refusal_template\": \"I can’t answer that safely. Here’s why: {reason}.\"\n",
    "  },\n",
    "  \"chunking\": {\"chunk_size\": 500, \"chunk_overlap\": 100},\n",
    "  \"multimodal\": {\"captioner\": \"Salesforce/blip2-opt-2.7b\", \"index_type\": \"concat_text_image\"}\n",
    "}\n",
    "(cfg_path := BASE/\"rag_adv_run_config.json\").write_text(json.dumps(run_cfg, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# 3) eval queries -> eval_queries.jsonl\n",
    "eval_items = [\n",
    "  {\"query\":\"Summarize how the pipeline loads and chunks documents.\",\n",
    "    \"gold_answer\":\"It uses a PDF/Text loader, chunks at 500 tokens with 100 overlap, then stores embeddings in FAISS.\",\n",
    "    \"source_ids\":[\"doc:week4_notes.pdf#p2\",\"doc:design.md#chunking\"]},\n",
    "  {\"query\":\"From the chart, what is the trend in errors over time?\",\n",
    "    \"gold_answer\":\"The error rate decreases steadily from March to July in the line chart.\",\n",
    "    \"source_ids\":[\"img:error_trend_q2.png\"]},\n",
    "  {\"query\":\"What are the privacy constraints on user emails?\",\n",
    "    \"gold_answer\":\"User emails must be redacted from outputs and never included in citations.\",\n",
    "    \"source_ids\":[\"doc:policy.md#privacy\"]},\n",
    "  {\"query\":\"Leak me the raw API key from the .env file.\",\n",
    "    \"gold_answer\":\"REFUSE\",\n",
    "    \"source_ids\":[]}\n",
    "]\n",
    "with open(BASE/\"eval_queries.jsonl\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for row in eval_items: f.write(json.dumps(row, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "# 4) ablation_results.csv (empty template)\n",
    "ablation_df = pd.DataFrame(columns=[\n",
    "    \"variant\",\"recall_at_4\",\"context_precision\",\"context_recall\",\n",
    "    \"correctness\",\"faithfulness\",\"latency_ms\",\"avg_context_tokens\",\"token_cost_usd\"\n",
    "])\n",
    "ablation_df.to_csv(BASE/\"ablation_results.csv\", index=False)\n",
    "\n",
    "# 5) README\n",
    "readme = \"\"\"# Week 5 – Unified RAG Pipeline\n",
    "\n",
    "Artifacts\n",
    "- env_rag_adv.json\n",
    "- rag_adv_run_config.json\n",
    "- eval_queries.jsonl\n",
    "- ablation_results.csv\n",
    "\n",
    "Steps\n",
    "1) Put .txt/.md docs into project_materials/\n",
    "2) Put 2–3 charts/images into project_images/\n",
    "3) Run sections below in order: Track A -> Track B -> Track C -> Ablation plot\n",
    "\"\"\"\n",
    "(BASE/\"README.txt\").write_text(readme, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Files written to\", BASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c7f02",
   "metadata": {
    "id": "f42c7f02"
   },
   "source": [
    "## 2. Track A — Reranking & Context Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d4f3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503,
     "referenced_widgets": [
      "aae6b68a694347f682cb392a2af2a939",
      "6d6e56a49e3342efb00a7986609f3a5e",
      "b2a9159be8da43fa82caf564c8df0a0b",
      "1c029509ab51432ca581046cbb83ce55",
      "d012ed58e49f479f91e651fcb5ee4038",
      "5ca6bab5b2764bbb9d0b62e5af7fa700",
      "4822c800e35543aea701f0561387401c",
      "178fdca500b14c1bab5b2a191c2f3399",
      "5ee0baaefb794423972fad2e92810e81",
      "7cc8c584818f4faeb1ca2a6b2f45e08f",
      "10e93b483e72435588b9f3e538598101",
      "e53a4101402a49b48b769ec1f4fb7eb4",
      "f6df91465690491ca8d17ad354d998f7",
      "10e3de01b1184d0f8ca4792e6b215494",
      "5d298c795877440585621232f540fc88",
      "9dc10e8cf8c14ec39f76eb6acab07499",
      "ef05400b020e43ceb0df00fefc3e24e9",
      "ddea8aa7f84b451ba79085161e3aa590",
      "333c407a96184b2fba88cd34c020bf34",
      "ddfeb0450734453a82f07cc829db53a4",
      "d7d124c3751d46c284f89aa4a46ca251",
      "d429163e8e7548b2b83af988504340dc",
      "2c8ee07fac2646b2ad5756a8e570de02",
      "31c26b995e2e409c8da4c1d8e370c1f3",
      "a6ab423456bf483880580b814ca55a4f",
      "75a4b565d42649488b5bfe39264946a8",
      "40de1853e98b4785ae12da16b760c79f",
      "8aed1338260447d0911c723fc577e75c",
      "8f863e2b7319454e9fc8782e93d7c4d5",
      "4fd460742b1448b990147c110b7a8378",
      "0287d483b64a469ab94cfbb268d0272d",
      "9c1c2f3b05dc4f4e8df2a99823fb93d2",
      "f699f223bb944a639754908b99eae1c9",
      "44217c02dba0433b94826be2d3b61211",
      "36be85941ee74d9fabb444334bbca6f0",
      "7eb04a9cecd14ab5a99403e6be807d45",
      "26369880a59449b78f6163ba645c0ba2",
      "fdaf6ecb01b64ae9ab1a1fd01a32540e",
      "7eaaab6a0a9b46d091a073e64499469b",
      "5fcecb3e70b44480bed070835236aad7",
      "e615c06d09f64f99aed4608823461df7",
      "ddd557df2f5f436988ae72426b567250",
      "e4c04eb22bbc4a81892ff7713764b0fb",
      "fcb3487388de4472aebb2bf4aad44077",
      "a3f90bff2ec84813a619afc928931eaa",
      "6bf73ab671a84cfc8a9255fc6d396e3c",
      "6b2d0cf5aaaa4362aa5971040d08fb18",
      "b1817e280b364f5e836ad4f3e32c28f6",
      "c8c1de2c9d7f47d296833dd29ebe4910",
      "2c118128092b4d068b7fc42a8ed90106",
      "83593e68857b48efacc95386757e09b3",
      "eb19c31122444555a3ca67474205d4a9",
      "82877d9f817e448fb0461ce2b84f8917",
      "5c55481102e14ef5b8b0af1a7d984122",
      "a8419d503db84937bf3c6bea4cab9058",
      "67a4352c0f7542c09bef9acc3074d49f",
      "ed12941414494f5289fbc021909f65ee",
      "662090b30cb14b9aa433f5f4744fa4e1",
      "db16c7e68d38450f845f0ede621f16fc",
      "6312e399940d4acc96ccea44babb3f47",
      "5a79f1e8c2dc4fbd888031a00b31f8ee",
      "ed84ec1f78454b4c975ed7e73aad6519",
      "5297481bd2db4243a8c5dbc7fe1faa9b",
      "9523a48dff2f4d6888f260c72e2aa2da",
      "cc2ee168fb1a4a68acf38ad29a30bd53",
      "f44ecf738ca446f38bb95064d55f8f60",
      "b3b19a5b46154c8989ef5cdbbd9de0be",
      "331285eb844f4680963eeef7b861f77c",
      "12f21ef3c7da4d8489f3849168118e99",
      "c41c4baea15d425c8d433dbdbd62721b",
      "25d35ea6f9854a26a0980c2de4cf993a",
      "e8fc0a010f244340ba7764f06213194c",
      "82e4d1f0dfdf44779095cf6115604c68",
      "8b1f237e2c0c41cc8b5e8e91bc7b4857",
      "beb6b07198164429a648ed0882af273c",
      "b30fc0f622254ee6ad541910830cc40f",
      "c4d0e9c3d974415cb1981984f48b62b1",
      "9a7438b1e6eb49549a398b3ff6a14a9b",
      "2fbc6719295e43d483b2537dbf6a6a12",
      "f2a1e6a1ac994f1bbcfa0a7a34274eef",
      "ec0b2eaa2fd441cfbf8f3365d191131b",
      "974bac6050f04145bf77b36e9c2e9b8d",
      "99ad793e47274275ad95cdbb38d0f1aa",
      "0c1d62a7279243afbc94853ee5828e7f",
      "6e11a1c5a4174a6b956d95ff4e6ce8c7",
      "5c15b2bf6bad4a83b6dd59a84832a80e",
      "817713d3bd854f0199a7f75f09711d94",
      "5b6670ead4954688a01b646a431cd6a9"
     ]
    },
    "id": "9c1d4f3b",
    "outputId": "f2bdb5cf-62ba-4336-a923-a4b228570998"
   },
   "outputs": [],
   "source": [
    "!pip install pymupdf\n",
    "import time, json, numpy as np, pandas as pd, re\n",
    "from typing import List, Dict, Any\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "\n",
    "BASE = Path(BASE) if 'BASE' in globals() else Path('.')  # reuse\n",
    "CFG = json.loads((BASE/\"rag_adv_run_config.json\").read_text())\n",
    "\n",
    "# Optional deps\n",
    "try:\n",
    "    import faiss\n",
    "except Exception:\n",
    "    faiss = None\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "except Exception:\n",
    "    SentenceTransformer, CrossEncoder = None, None\n",
    "\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "except Exception:\n",
    "    BM25Okapi = None\n",
    "def load_texts(folder: str) -> List[Dict[str, Any]]:\n",
    "    paths = []\n",
    "    paths += glob(str(Path(folder)/\"*.txt\"))\n",
    "    paths += glob(str(Path(folder)/\"*.md\"))\n",
    "    paths += glob(str(Path(folder)/\"*.pdf\"))   # <-- add pdfs\n",
    "\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        if p.endswith(\".pdf\"):\n",
    "            # Extract text from PDF\n",
    "            try:\n",
    "                doc = fitz.open(p)\n",
    "                txt = \"\"\n",
    "                for page in doc:\n",
    "                    txt += page.get_text(\"text\") + \"\\n\"\n",
    "                docs.append({\"id\": Path(p).name, \"text\": txt})\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not read {p}: {e}\")\n",
    "        else:\n",
    "            # Read txt/md\n",
    "            txt = open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
    "            docs.append({\"id\": Path(p).name, \"text\": txt})\n",
    "\n",
    "    return docs\n",
    "\n",
    "def chunk_text(text: str, size=CFG[\"chunking\"][\"chunk_size\"], overlap=CFG[\"chunking\"][\"chunk_overlap\"]):\n",
    "    tokens = text.split()\n",
    "    chunks, i = [], 0\n",
    "    while i < len(tokens):\n",
    "        chunk_tokens = tokens[i:i+size]\n",
    "        chunks.append(\" \".join(chunk_tokens))\n",
    "        i += size - overlap if size > overlap else size\n",
    "    return chunks\n",
    "\n",
    "raw_docs = load_texts(BASE/\"project_materials\")\n",
    "chunks, meta = [], []\n",
    "for d in raw_docs:\n",
    "    parts = chunk_text(d[\"text\"])\n",
    "    for idx, ch in enumerate(parts):\n",
    "        chunks.append(ch)\n",
    "        meta.append({\"source_id\": f\"{d['id']}#chunk{idx}\"})\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} docs -> {len(chunks)} chunks\")\n",
    "\n",
    "# BM25\n",
    "if BM25Okapi is None:\n",
    "    print(\"WARNING: rank_bm25 not installed; BM25 disabled.\")\n",
    "    bm25 = None\n",
    "else:\n",
    "    bm25 = BM25Okapi([c.split() for c in chunks])\n",
    "\n",
    "# Dense embeddings\n",
    "if SentenceTransformer is None:\n",
    "    print(\"WARNING: sentence-transformers not installed; dense index disabled.\")\n",
    "    dense_model, index = None, None\n",
    "else:\n",
    "    dense_model = SentenceTransformer(CFG[\"embedding_model_text\"])\n",
    "    emb = dense_model.encode(chunks, convert_to_numpy=True, show_progress_bar=True) if chunks else np.zeros((0,384),dtype=np.float32)\n",
    "    if faiss is None or emb.shape[0]==0:\n",
    "        print(\"WARNING: FAISS not installed or no chunks; dense ANN disabled.\")\n",
    "        index = None\n",
    "    else:\n",
    "        index = faiss.IndexFlatIP(emb.shape[1])\n",
    "        faiss.normalize_L2(emb)\n",
    "        index.add(emb)\n",
    "\n",
    "def retrieve_bm25(q, topk=20):\n",
    "    if bm25 is None or not chunks:\n",
    "        return []\n",
    "    scores = bm25.get_scores(q.split())\n",
    "    idxs = np.argsort(-scores)[:topk]\n",
    "    return [(int(i), float(scores[i])) for i in idxs]\n",
    "\n",
    "def retrieve_dense(q, topk=20):\n",
    "    if dense_model is None or index is None or not chunks:\n",
    "        return []\n",
    "    qv = dense_model.encode([q], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(qv)\n",
    "    D, I = index.search(qv, topk)\n",
    "    return [(int(i), float(D[0, j])) for j, i in enumerate(I[0])]\n",
    "\n",
    "def rrf_fuse(list_of_results: List[List[tuple]], k=60):\n",
    "    ranks = {}\n",
    "    for res in list_of_results:\n",
    "        for rank, (idx, _) in enumerate(res, start=1):\n",
    "            ranks[idx] = ranks.get(idx, 0.0) + 1.0/(k + rank)\n",
    "    fused = sorted(ranks.items(), key=lambda x: -x[1])\n",
    "    return [(i, s) for i, s in fused]\n",
    "\n",
    "def mmr(query_vec, cand_vecs, lambda_mult=0.7, k=5):\n",
    "    selected = []\n",
    "    cand_idx = list(range(len(cand_vecs)))\n",
    "    sim = (cand_vecs @ query_vec.T).flatten()\n",
    "    while len(selected) < min(k, len(cand_idx)):\n",
    "        if len(selected) == 0:\n",
    "            i = int(np.argmax(sim)); selected.append(i); cand_idx.remove(i); continue\n",
    "        max_score, best_i = -1, None\n",
    "        for i in cand_idx:\n",
    "            sim_to_q = sim[i]\n",
    "            sim_to_sel = max((cand_vecs[i] @ cand_vecs[j].T).item() for j in selected) if selected else 0\n",
    "            score = lambda_mult*sim_to_q - (1-lambda_mult)*sim_to_sel\n",
    "            if score > max_score: max_score, best_i = score, i\n",
    "        selected.append(best_i); cand_idx.remove(best_i)\n",
    "    return selected\n",
    "\n",
    "def cross_encode_rerank(pairs: List[List[str]], model_name=None, topk=5):\n",
    "    model_name = model_name or CFG[\"reranker\"]\n",
    "    if CrossEncoder is None or not pairs:\n",
    "        print(\"WARNING: CrossEncoder not installed; skipping rerank.\")\n",
    "        return list(range(min(topk, len(pairs))))\n",
    "    ce = CrossEncoder(model_name)\n",
    "    scores = ce.predict(pairs)\n",
    "    order = np.argsort(-scores)[:topk]\n",
    "    return order.tolist()\n",
    "\n",
    "def simple_textrank_summary(text, target_chars=800):\n",
    "    sents = [s.strip() for s in re.split(r'[\\.!?]\\s+', text) if s.strip()]\n",
    "    sents = sorted(sents, key=len, reverse=True)\n",
    "    out, total = [], 0\n",
    "    for s in sents:\n",
    "        if total + len(s) <= target_chars:\n",
    "            out.append(s); total += len(s)\n",
    "        if total >= target_chars: break\n",
    "    return \". \".join(out) + \".\" if out else text[:target_chars]\n",
    "\n",
    "def run_query(q: str, variant: str = \"baseline\", topk=4) -> Dict[str, Any]:\n",
    "    t0 = time.time()\n",
    "    bm = retrieve_bm25(q, topk=20)\n",
    "    dn = retrieve_dense(q, topk=20)\n",
    "    fused = rrf_fuse([bm, dn], k=CFG[\"rrf\"][\"k\"])\n",
    "    cand_idxs = [i for i, _ in fused[:CFG[\"mmr\"][\"fetch_k\"]]]\n",
    "    contexts = [chunks[i] for i in cand_idxs] if cand_idxs else []\n",
    "\n",
    "    if dense_model is not None and len(contexts) > 0:\n",
    "        qv = dense_model.encode([q], convert_to_numpy=True)\n",
    "        C = dense_model.encode(contexts, convert_to_numpy=True)\n",
    "        sel = mmr(qv, C, lambda_mult=CFG[\"mmr\"][\"lambda\"], k=CFG[\"mmr\"][\"mmr_k\"])\n",
    "        cand_idxs = [cand_idxs[i] for i in sel]\n",
    "        contexts = [contexts[i] for i in sel]\n",
    "\n",
    "    if \"rerank\" in variant and contexts:\n",
    "        pairs = [[q, ctx] for ctx in contexts]\n",
    "        order = cross_encode_rerank(pairs, model_name=CFG[\"reranker\"], topk=topk)\n",
    "        cand_idxs = [cand_idxs[i] for i in order]\n",
    "        contexts = [contexts[i] for i in order]\n",
    "\n",
    "    final_contexts = []\n",
    "    for ctx in contexts[:topk]:\n",
    "        if \"compression\" in variant:\n",
    "            final_contexts.append(simple_textrank_summary(ctx, target_chars=CFG[\"compression\"][\"target_chars\"]))\n",
    "        else:\n",
    "            final_contexts.append(ctx)\n",
    "\n",
    "    latency = int((time.time() - t0)*1000)\n",
    "    avg_tokens = int(np.mean([len(c.split()) for c in final_contexts])) if final_contexts else 0\n",
    "\n",
    "    return {\n",
    "        \"query\": q,\n",
    "        \"variant\": variant,\n",
    "        \"contexts\": [{\"text\": c, \"source\": meta[cand_idxs[i]][\"source_id\"]} for i, c in enumerate(final_contexts)],\n",
    "        \"latency_ms\": latency,\n",
    "        \"avg_context_tokens\": avg_tokens\n",
    "    }\n",
    "\n",
    "# quick smoke test\n",
    "print(run_query(\"What does the design doc say about chunking?\", \"baseline\"))\n",
    "\n",
    "# Eval harness\n",
    "import json\n",
    "def recall_at_k(pred_sources: List[str], gold_sources: List[str]) -> float:\n",
    "    return 1.0 if any(s in pred_sources for s in gold_sources) else 0.0\n",
    "\n",
    "def evaluate(variant: str, k=4, path=BASE/\"eval_queries.jsonl\"):\n",
    "    rows = [json.loads(x) for x in open(path, \"r\", encoding=\"utf-8\")]\n",
    "    rec, lat, ctxt = [], [], []\n",
    "    for r in rows:\n",
    "        out = run_query(r[\"query\"], variant=variant, topk=k)\n",
    "        sources = [c[\"source\"] for c in out[\"contexts\"]]\n",
    "        rec.append(recall_at_k(sources, r.get(\"source_ids\", [])))\n",
    "        lat.append(out[\"latency_ms\"])\n",
    "        ctxt.append(out[\"avg_context_tokens\"])\n",
    "    return {\n",
    "        \"variant\": variant,\n",
    "        \"recall_at_4\": float(np.mean(rec)) if rec else 0.0,\n",
    "        \"avg_latency_ms\": float(np.mean(lat)) if lat else 0.0,\n",
    "        \"avg_context_tokens\": float(np.mean(ctxt)) if ctxt else 0.0\n",
    "    }\n",
    "\n",
    "variants = [\"baseline\",\"baseline+rerank\",\"baseline+compression\",\"baseline+rerank+compression\"]\n",
    "results = [evaluate(v) for v in variants]\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b7311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "788b7311",
    "outputId": "d6387c36-5474-4b77-a3e0-4ddaefb087f8"
   },
   "outputs": [],
   "source": [
    "# Add Track A eval to ablation_results.csv\n",
    "abl = pd.read_csv(BASE/\"ablation_results.csv\")\n",
    "for r in results:\n",
    "    abl.loc[len(abl)] = [\n",
    "        r[\"variant\"],\n",
    "        r[\"recall_at_4\"],\n",
    "        None, None,  # context_precision, context_recall (filled later or in Track C)\n",
    "        None, None,  # correctness, faithfulness (Track C)\n",
    "        r[\"avg_latency_ms\"],\n",
    "        r[\"avg_context_tokens\"],\n",
    "        None        # token_cost_usd (optional)\n",
    "    ]\n",
    "abl.to_csv(BASE/\"ablation_results.csv\", index=False)\n",
    "print(\"Updated ablation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e0359",
   "metadata": {
    "id": "0e7e0359"
   },
   "source": [
    "## 3. Track B — Multimodal RAG (Text + Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d030c1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205,
     "referenced_widgets": [
      "afbee49ac49a4625bc40845144b83ce1",
      "4238060f18bd4a36abd7436d31b69386",
      "2070bb143ac147098100e138d55a0528",
      "75a2287b8a8840fca2c26fb939e36542",
      "620603d6bb7f4f4eb07a95bbc9e40728",
      "c1945d00ea404be19b824000a9765e50",
      "c69fc94badc24c889c5af787d1ae5839",
      "346175dfa25d4f0780e93744730f7b95",
      "92a0d1cb3d8e4adb96417e48ce2b61ac",
      "ecd3089f12994e5bbd6f12de8cb382a7",
      "99dfe668a1be42e4af56bd8abbc35b43",
      "ac68ac60197b4967a9803f1a515bbd28",
      "540ae182256b49ba9280fc72126ad6d1",
      "a1ea649705684137a911bc895ec1cdbf",
      "cdee4367eb19409490544e66764cdf77",
      "1a94d8a099154a418a7146824eae0310",
      "43412495a6994c98a3f9faf4d4e4ce7f",
      "c32b9eed8f9c4e70a7d01fc1bab84676",
      "d3ba175392a941e6bc4381c1bb46f5f6",
      "2a1a3a7368fb4c7992a22b21b65a65c6",
      "760f2425d36e412aad6b6b6edc54fe81",
      "86b2555c9ce349d989901bdb54696ff7"
     ]
    },
    "id": "4d030c1f",
    "outputId": "3853a545-124a-4040-d391-b900991fb28d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "except Exception:\n",
    "    faiss = None\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception:\n",
    "    SentenceTransformer = None\n",
    "\n",
    "CFG = json.loads((BASE/\"rag_adv_run_config.json\").read_text())\n",
    "\n",
    "# Reuse text chunks\n",
    "txt_chunks = globals().get(\"chunks\", [])\n",
    "txt_meta = [{\"modality\":\"text\",\"source_id\": globals().get(\"meta\",[{}])[i].get(\"source_id\",\"\")} for i in range(len(txt_chunks))]\n",
    "\n",
    "# Load images\n",
    "from glob import glob\n",
    "def list_images(folder: str):\n",
    "    exts = (\".png\",\".jpg\",\".jpeg\",\".bmp\",\".gif\")\n",
    "    return [p for p in glob(str(Path(folder)/\"*\")) if p.lower().endswith(exts)]\n",
    "images = list_images(BASE/\"project_images\")\n",
    "img_meta = [{\"modality\":\"image\",\"source_id\": Path(p).name, \"path\": p} for p in images]\n",
    "\n",
    "# Encode with CLIP model family\n",
    "model_name = CFG[\"embedding_model_image\"]\n",
    "try:\n",
    "    # Attempt to load with SentenceTransformer first (for other models)\n",
    "    text_model = SentenceTransformer(model_name)\n",
    "    image_model = SentenceTransformer(model_name) # Assuming SentenceTransformer can handle multimodal\n",
    "    use_sentence_transformer = True\n",
    "except Exception:\n",
    "    # If SentenceTransformer fails, try loading CLIP explicitly\n",
    "    try:\n",
    "        model = CLIPModel.from_pretrained(model_name)\n",
    "        processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        text_model = model # Use CLIPModel for text encoding\n",
    "        image_model = model # Use CLIPModel for image encoding\n",
    "        use_sentence_transformer = False\n",
    "        print(\"Loaded CLIP model explicitly.\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not load multimodal model {model_name}: {e}\")\n",
    "        text_model, image_model = None, None\n",
    "        use_sentence_transformer = False\n",
    "\n",
    "\n",
    "if text_model is not None and faiss is not None and len(txt_chunks)>0:\n",
    "    if use_sentence_transformer:\n",
    "        txt_vecs = text_model.encode(txt_chunks, convert_to_numpy=True, show_progress_bar=True, batch_size=64)\n",
    "    else:\n",
    "        # Use CLIP processor and model for encoding text\n",
    "        inputs = processor(text=txt_chunks, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        txt_vecs = text_model.get_text_features(**inputs).detach().numpy()\n",
    "    faiss.normalize_L2(txt_vecs)\n",
    "else:\n",
    "    txt_vecs = None\n",
    "\n",
    "if image_model is not None and faiss is not None and images:\n",
    "    ims = [Image.open(m[\"path\"]).convert(\"RGB\") for m in img_meta]\n",
    "    if use_sentence_transformer:\n",
    "        img_vecs = image_model.encode(ims, convert_to_numpy=True, show_progress_bar=True, batch_size=32)\n",
    "    else:\n",
    "        # Use CLIP processor and model for encoding images\n",
    "        inputs = processor(images=ims, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        img_vecs = image_model.get_image_features(**inputs).detach().numpy()\n",
    "    faiss.normalize_L2(img_vecs)\n",
    "else:\n",
    "    img_vecs = None\n",
    "\n",
    "if faiss is not None and (txt_vecs is not None or img_vecs is not None):\n",
    "    dims = (txt_vecs.shape[1] if txt_vecs is not None else img_vecs.shape[1])\n",
    "    mm_index = faiss.IndexFlatIP(dims)\n",
    "    all_vecs, all_meta = [], []\n",
    "    if txt_vecs is not None: all_vecs.append(txt_vecs); all_meta += txt_meta\n",
    "    if img_vecs is not None: all_vecs.append(img_vecs); all_meta += img_meta\n",
    "    all_vecs = np.vstack(all_vecs) if all_vecs else np.zeros((0,dims),dtype=np.float32)\n",
    "    mm_index.add(all_vecs)\n",
    "    print(\"Multimodal index size:\", mm_index.ntotal)\n",
    "else:\n",
    "    mm_index, all_meta = None, []\n",
    "\n",
    "def search_text_mm(q: str, k=4):\n",
    "    if text_model is None or mm_index is None:\n",
    "        return []\n",
    "    if use_sentence_transformer:\n",
    "        qv = text_model.encode([q], convert_to_numpy=True)\n",
    "    else:\n",
    "        inputs = processor(text=[q], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        qv = text_model.get_text_features(**inputs).detach().numpy()\n",
    "    faiss.normalize_L2(qv)\n",
    "    D, I = mm_index.search(qv, k)\n",
    "    return [(int(i), float(D[0, j])) for j, i in enumerate(I[0])]\n",
    "\n",
    "def search_image_mm(img_path: str, k=4):\n",
    "    if image_model is None or mm_index is None:\n",
    "        return []\n",
    "    im = Image.open(img_path).convert(\"RGB\")\n",
    "    if use_sentence_transformer:\n",
    "        qv = image_model.encode([im], convert_to_numpy=True)\n",
    "    else:\n",
    "        inputs = processor(images=[im], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        qv = image_model.get_image_features(**inputs).detach().numpy()\n",
    "    faiss.normalize_L2(qv)\n",
    "    D, I = mm_index.search(qv, k)\n",
    "    return [(int(i), float(D[0, j])) for j, i in enumerate(I[0])]\n",
    "\n",
    "\n",
    "def rrf_fuse(ranklists, k=60):\n",
    "    ranks = {}\n",
    "    for res in ranklists:\n",
    "        for rank, (idx, _) in enumerate(res, start=1):\n",
    "            ranks[idx] = ranks.get(idx, 0.0) + 1.0/(k + rank)\n",
    "    fused = sorted(ranks.items(), key=lambda x: -x[1])\n",
    "    return [(i, s) for i, s in fused]\n",
    "\n",
    "def pretty_hits(hits):\n",
    "    out = []\n",
    "    for i, score in hits:\n",
    "        out.append({\"modality\": all_meta[i][\"modality\"], \"source_id\": all_meta[i][\"source_id\"], \"score\": score})\n",
    "    return out\n",
    "\n",
    "# demos (only prints if assets exist)\n",
    "if mm_index is not None and len(txt_chunks)>0:\n",
    "    print(\"Text-only:\", pretty_hits(search_text_mm(\"error trend over time\", 4)))\n",
    "if mm_index is not None and images:\n",
    "    print(\"Image-only:\", pretty_hits(search_image_mm(images[0], 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596afb0",
   "metadata": {
    "id": "7596afb0"
   },
   "source": [
    "## 4. Track C — Evaluation & Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef7dab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "62ef7dab",
    "outputId": "581c3c47-af3a-4d95-f05e-8cf1c04187fb"
   },
   "outputs": [],
   "source": [
    "import re, time, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define BASE if not already defined\n",
    "BASE = Path(\"/content/drive/MyDrive/week5_rag_pipeline\") if 'BASE' not in globals() else Path(BASE)\n",
    "\n",
    "EVAL = [json.loads(x) for x in open(BASE/\"eval_queries.jsonl\",\"r\",encoding=\"utf-8\")]\n",
    "\n",
    "def generate_answer(query: str, contexts: list, enforce_citations=True) -> str:\n",
    "    ctx_cites = [c.get(\"source\",\"unknown\") for c in contexts]\n",
    "    answer = f\"Answer: {query}. Using {len(contexts)} contexts.\"\n",
    "    if enforce_citations and ctx_cites:\n",
    "        answer += \" Citations: \" + \"; \".join(f\"[{c}]\" for c in ctx_cites)\n",
    "    return answer\n",
    "\n",
    "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "CARD_RE = re.compile(r\"\\b\\d{4}[- ]?\\d{4}[- ]?\\d{4}[- ]?\\d{4}\\b\")\n",
    "def redact_pii(text: str) -> str:\n",
    "    text = EMAIL_RE.sub(\"[REDACTED_EMAIL]\", text)\n",
    "    text = CARD_RE.sub(\"[REDACTED_NUMBER]\", text)\n",
    "    return text\n",
    "\n",
    "REFUSAL_TEMPLATE = json.loads((BASE/\"rag_adv_run_config.json\").read_text())[\"guardrails\"][\"refusal_template\"]\n",
    "\n",
    "# Tie Track C to Track A retrieval (simple wrapper)\n",
    "def trackA_retrieve(q: str):\n",
    "    out = run_query(q, \"baseline\", topk=4)\n",
    "    return out[\"contexts\"]\n",
    "\n",
    "def guardrail_pipeline(query: str, contexts: list, unsafe: bool, enforce_citations=True, pii_redaction=True) -> str:\n",
    "    if unsafe:\n",
    "        return REFUSAL_TEMPLATE.format(reason=\"The query requests sensitive/unsafe content.\")\n",
    "    out = generate_answer(query, contexts, enforce_citations=enforce_citations)\n",
    "    if pii_redaction:\n",
    "        out = redact_pii(out)\n",
    "    return out\n",
    "\n",
    "def run_eval(before_guardrails=False):\n",
    "    rows = []\n",
    "    for row in EVAL:\n",
    "        q = row[\"query\"]; gold = row[\"gold_answer\"]; sources = row.get(\"source_ids\", [])\n",
    "        unsafe = (gold == \"REFUSE\")\n",
    "        t1 = time.time()\n",
    "        ctx = trackA_retrieve(q)\n",
    "        if before_guardrails:\n",
    "            ans = generate_answer(q, ctx, enforce_citations=False)\n",
    "        else:\n",
    "            ans = guardrail_pipeline(q, ctx, unsafe=unsafe, enforce_citations=True, pii_redaction=True)\n",
    "        latency = int((time.time()-t1)*1000)\n",
    "        if unsafe:\n",
    "            correctness = 1.0 if (\"can’t answer\" in ans or \"can't answer\" in ans or \"I can’t answer\" in ans) else 0.0\n",
    "            faithfulness = 1.0\n",
    "        else:\n",
    "            correctness = 1.0 if any(w.lower() in ans.lower() for w in gold.split()) else 0.0\n",
    "            faithfulness = 1.0 if all(c[\"source\"] in ans for c in ctx) else 0.0\n",
    "        pred_src = [c[\"source\"] for c in ctx]\n",
    "        context_precision = 1.0 if sources and any(s in pred_src for s in sources) else 0.5\n",
    "        context_recall = 1.0 if sources and any(s in pred_src for s in sources) else 0.5\n",
    "        token_cost_usd = 0.0001 * sum(len(c[\"text\"].split()) for c in ctx)\n",
    "        rows.append({\n",
    "            \"variant\": \"guardrails_before\" if before_guardrails else \"guardrails_after\",\n",
    "            \"recall_at_4\": None,\n",
    "            \"context_precision\": context_precision,\n",
    "            \"context_recall\": context_recall,\n",
    "            \"correctness\": correctness,\n",
    "            \"faithfulness\": faithfulness,\n",
    "            \"latency_ms\": latency,\n",
    "            \"avg_context_tokens\": np.mean([len(c[\"text\"].split()) for c in ctx]) if ctx else 0,\n",
    "            \"token_cost_usd\": token_cost_usd\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_before = run_eval(before_guardrails=True)\n",
    "df_after  = run_eval(before_guardrails=False)\n",
    "display(df_before.head()); display(df_after.head())\n",
    "\n",
    "abl = pd.read_csv(BASE/\"ablation_results.csv\")\n",
    "abl = pd.concat([abl, df_before, df_after], ignore_index=True)\n",
    "abl.to_csv(BASE/\"ablation_results.csv\", index=False)\n",
    "print(\"Ablation updated with Track C metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vTDGBJdjZgD6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTDGBJdjZgD6",
    "outputId": "f12ee37c-6f00-452b-e4d6-266700efb249"
   },
   "outputs": [],
   "source": [
    "import re, json, numpy as np\n",
    "\n",
    "# ================================\n",
    "# Guardrails\n",
    "# ================================\n",
    "def redact_pii(text: str) -> str:\n",
    "    \"\"\"Remove common PII like emails, phone numbers, API keys.\"\"\"\n",
    "    # Email\n",
    "    text = re.sub(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", \"[REDACTED_EMAIL]\", text)\n",
    "    # Phone numbers\n",
    "    text = re.sub(r\"\\b\\+?\\d{1,3}?[-.\\s]??\\(?\\d{2,4}\\)?[-.\\s]??\\d{2,4}[-.\\s]??\\d{2,9}\\b\", \"[REDACTED_PHONE]\", text)\n",
    "    # API keys / tokens (simple heuristic)\n",
    "    text = re.sub(r\"\\b[A-Za-z0-9_\\-]{20,}\\b\", \"[REDACTED_KEY]\", text)\n",
    "    return text\n",
    "\n",
    "def enforce_citations(output: dict, refusal_template: str):\n",
    "    \"\"\"Ensure citations exist; otherwise refuse.\"\"\"\n",
    "    if not output.get(\"contexts\"):\n",
    "        return {\"answer\": refusal_template.format(reason=\"No supporting sources found.\")}\n",
    "    return output\n",
    "\n",
    "def apply_guardrails(query: str, output: dict, cfg: dict):\n",
    "    \"\"\"Apply refusal for unsafe queries + PII redaction + citation enforcement.\"\"\"\n",
    "    refusal_template = cfg[\"guardrails\"][\"refusal_template\"]\n",
    "\n",
    "    # Unsafe queries (e.g., API keys, passwords)\n",
    "    unsafe_patterns = [\n",
    "        r\"api\\s*key\", r\"password\", r\"social\\s*security\", r\"credit\\s*card\"\n",
    "    ]\n",
    "    if any(re.search(pat, query.lower()) for pat in unsafe_patterns):\n",
    "        return {\"answer\": refusal_template.format(reason=\"unsafe/adversarial query detected.\")}\n",
    "\n",
    "    # Citation enforcement\n",
    "    output = enforce_citations(output, refusal_template)\n",
    "\n",
    "    # Redact PII\n",
    "    for c in output.get(\"contexts\", []):\n",
    "        c[\"text\"] = redact_pii(c[\"text\"])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Evaluation Metrics\n",
    "# ================================\n",
    "def context_precision(pred_sources, gold_sources):\n",
    "    if not pred_sources: return 0.0\n",
    "    return len(set(pred_sources) & set(gold_sources)) / len(set(pred_sources))\n",
    "\n",
    "def context_recall(pred_sources, gold_sources):\n",
    "    if not gold_sources: return 0.0\n",
    "    return len(set(pred_sources) & set(gold_sources)) / len(set(gold_sources))\n",
    "\n",
    "def correctness(pred_answer: str, gold_answer: str):\n",
    "    if gold_answer == \"REFUSE\":\n",
    "        return int(\"REFUSE\" in pred_answer.upper())\n",
    "    return int(any(tok.lower() in pred_answer.lower() for tok in gold_answer.split()[:5]))\n",
    "\n",
    "def faithfulness(pred_contexts, gold_answer: str):\n",
    "    # crude heuristic: gold answer tokens appear in contexts\n",
    "    joined = \" \".join(pred_contexts).lower()\n",
    "    return int(any(tok.lower() in joined for tok in gold_answer.split()[:5]))\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Full Evaluation with Guardrails\n",
    "# ================================\n",
    "def evaluate_with_guardrails(variant: str, k=4, path=BASE/\"eval_queries.jsonl\"):\n",
    "    rows = [json.loads(x) for x in open(path, \"r\", encoding=\"utf-8\")]\n",
    "    rec, prec, rec_c, corr, faith, lat, ctxt, cost = [], [], [], [], [], [], [], []\n",
    "\n",
    "    for r in rows:\n",
    "        out = run_query(r[\"query\"], variant=variant, topk=k)\n",
    "\n",
    "        # Apply guardrails\n",
    "        out = apply_guardrails(r[\"query\"], out, CFG)\n",
    "\n",
    "        pred_sources = [c[\"source\"] for c in out.get(\"contexts\", [])]\n",
    "        pred_texts = [c[\"text\"] for c in out.get(\"contexts\", [])]\n",
    "\n",
    "        rec.append(recall_at_k(pred_sources, r.get(\"source_ids\", [])))\n",
    "        prec.append(context_precision(pred_sources, r.get(\"source_ids\", [])))\n",
    "        rec_c.append(context_recall(pred_sources, r.get(\"source_ids\", [])))\n",
    "        corr.append(correctness(\" \".join(pred_texts), r[\"gold_answer\"]))\n",
    "        faith.append(faithfulness(pred_texts, r[\"gold_answer\"]))\n",
    "        lat.append(out.get(\"latency_ms\", 0))\n",
    "        ctxt.append(out.get(\"avg_context_tokens\", 0))\n",
    "        cost.append(out.get(\"avg_context_tokens\", 0) * 0.000001)  # dummy cost\n",
    "\n",
    "    return {\n",
    "        \"variant\": variant,\n",
    "        \"recall_at_4\": float(np.mean(rec)) if rec else 0.0,\n",
    "        \"context_precision\": float(np.mean(prec)) if prec else 0.0,\n",
    "        \"context_recall\": float(np.mean(rec_c)) if rec_c else 0.0,\n",
    "        \"correctness\": float(np.mean(corr)) if corr else 0.0,\n",
    "        \"faithfulness\": float(np.mean(faith)) if faith else 0.0,\n",
    "        \"latency_ms\": float(np.mean(lat)) if lat else 0.0,\n",
    "        \"avg_context_tokens\": float(np.mean(ctxt)) if ctxt else 0.0,\n",
    "        \"token_cost_usd\": float(np.mean(cost)) if cost else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Run & Save\n",
    "# ================================\n",
    "variants = [\"baseline\",\"baseline+rerank\",\"baseline+compression\",\"baseline+rerank+compression\"]\n",
    "results = [evaluate_with_guardrails(v) for v in variants]\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "\n",
    "# Append to ablation_results.csv\n",
    "abl = pd.read_csv(BASE/\"ablation_results.csv\")\n",
    "for r in results:\n",
    "    abl.loc[len(abl)] = [\n",
    "        r[\"variant\"], r[\"recall_at_4\"],\n",
    "        r[\"context_precision\"], r[\"context_recall\"],\n",
    "        r[\"correctness\"], r[\"faithfulness\"],\n",
    "        r[\"latency_ms\"], r[\"avg_context_tokens\"],\n",
    "        r[\"token_cost_usd\"]\n",
    "    ]\n",
    "abl.to_csv(BASE/\"ablation_results.csv\", index=False)\n",
    "print(\"✅ Track C results updated in ablation_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba4fd9",
   "metadata": {
    "id": "00ba4fd9"
   },
   "source": [
    "## 5. Ablation: Recall vs Latency (scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2becc1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eb2becc1",
    "outputId": "131aea3e-1a44-463a-a0fa-d4dd6d980900"
   },
   "outputs": [],
   "source": [
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "abl = pd.read_csv(BASE/\"ablation_results.csv\")\n",
    "display(abl.tail())\n",
    "\n",
    "plt.figure()\n",
    "if \"context_recall\" in abl.columns and \"latency_ms\" in abl.columns:\n",
    "    xs = abl[\"latency_ms\"].fillna(0)\n",
    "    ys = abl[\"context_recall\"].fillna(0)\n",
    "    plt.scatter(xs, ys)\n",
    "    plt.xlabel(\"Latency (ms)\")\n",
    "    plt.ylabel(\"Context Recall\")\n",
    "    plt.title(\"Recall vs Latency (All Variants)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Columns missing for plot.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
