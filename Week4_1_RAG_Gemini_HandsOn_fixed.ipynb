{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "kaZdbA0-ZvS3",
   "metadata": {
    "id": "kaZdbA0-ZvS3"
   },
   "source": [
    "# CS5588 — Week 4: **Pure RAG with Gemini API** (No LangChain)\n",
    "\n",
    "This notebook implements a minimal, production-style Retrieval-Augmented Generation (RAG) pipeline **using only the Gemini API** for both **embeddings** and **text generation**.\n",
    "\n",
    "**What you'll do**\n",
    "1. Install & setup (`google-generativeai`)\n",
    "2. Log environment → `env_rag.json`\n",
    "3. Load documents (PDF/TXT/MD)\n",
    "4. Chunk documents (default: `size=500`, `overlap=100`)\n",
    "5. Embed with **Gemini Embeddings** (`text-embedding-004`)\n",
    "6. Build a tiny vector index (NumPy; FAISS optional)\n",
    "7. Retrieve top-*k* chunks (cosine similarity)\n",
    "8. Generate answers with **Gemini 1.5 Flash** grounded in retrieved context\n",
    "9. Mini-experiment: chunk sensitivity (500/100 vs 300/50)\n",
    "10. Save reproducibility config → `rag_gemini_config.json`\n",
    "\n",
    "> **API key:** Set `GEMINI_API_KEY` in your environment before running:\n",
    "> - Colab: `import os; os.environ[\"GEMINI_API_KEY\"] = \"YOUR_KEY_HERE\"`\n",
    "> - Local: export in your shell or use `.env` loading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QZDubEx-ZvS7",
   "metadata": {
    "id": "QZDubEx-ZvS7"
   },
   "source": [
    "## 1) Install & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8suPdIxJZvS8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8suPdIxJZvS8",
    "outputId": "ed1d6eff-25f9-43d8-8620-f476ead2beee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing: ['google-generativeai>=0.7.2', 'PyPDF2>=3.0.1', 'numpy>=1.23.0']\n",
      "Installing: ['faiss-cpu>=1.8.0']\n",
      "✅ Setup cell finished.\n"
     ]
    }
   ],
   "source": [
    "# If running in Colab: install Google Generative AI client and helpers\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    print(\"Installing:\", pkgs)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs, check=True)\n",
    "\n",
    "try:\n",
    "    pip_install([\"google-generativeai>=0.7.2\", \"PyPDF2>=3.0.1\", \"numpy>=1.23.0\"])\n",
    "except Exception as e:\n",
    "    print(\"Install warning:\", e)\n",
    "\n",
    "# Optional FAISS (fast vector search); will fallback to pure NumPy if install fails\n",
    "try:\n",
    "    pip_install([\"faiss-cpu>=1.8.0\"])\n",
    "except Exception as e:\n",
    "    print(\"FAISS install skipped (optional):\", e)\n",
    "\n",
    "print(\"✅ Setup cell finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CpKIw9EmZvS9",
   "metadata": {
    "id": "CpKIw9EmZvS9"
   },
   "source": [
    "## 2) Log Environment → `env_rag.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aw99bm7DZvS-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aw99bm7DZvS-",
    "outputId": "34e49672-b367-4a47-c8c5-7751d10ff500"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3057984888.py:5: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp_utc\": datetime.datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"timestamp_utc\": \"2025-09-18T20:03:04.471191Z\",\n",
      "  \"python\": \"3.12.11\",\n",
      "  \"platform\": \"Linux-6.1.123+-x86_64-with-glibc2.35\",\n",
      "  \"google-generativeai\": \"0.8.5\",\n",
      "  \"numpy\": \"2.0.2\",\n",
      "  \"PyPDF2\": \"3.0.1\",\n",
      "  \"torch\": \"2.8.0+cu126\",\n",
      "  \"cuda_available\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json, platform, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "env = {\n",
    "    \"timestamp_utc\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"python\": platform.python_version(),\n",
    "    \"platform\": platform.platform(),\n",
    "}\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    env[\"google-generativeai\"] = getattr(genai, \"__version__\", \"unknown\")\n",
    "except Exception as e:\n",
    "    env[\"google-generativeai\"] = f\"unavailable ({e})\"\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    env[\"numpy\"] = np.__version__\n",
    "except Exception as e:\n",
    "    env[\"numpy\"] = f\"unavailable ({e})\"\n",
    "\n",
    "try:\n",
    "    import PyPDF2\n",
    "    env[\"PyPDF2\"] = PyPDF2.__version__\n",
    "except Exception as e:\n",
    "    env[\"PyPDF2\"] = f\"unavailable ({e})\"\n",
    "\n",
    "# Optional libs\n",
    "try:\n",
    "    import torch\n",
    "    env[\"torch\"] = torch.__version__\n",
    "    env[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
    "except Exception:\n",
    "    env[\"torch\"] = \"N/A\"\n",
    "\n",
    "Path(\"runs\").mkdir(exist_ok=True)\n",
    "with open(\"env_rag.json\", \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(json.dumps(env, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-uKsFas-ZvS-",
   "metadata": {
    "id": "-uKsFas-ZvS-"
   },
   "source": [
    "## 3) Load Documents (PDF/TXT/MD)\n",
    "\n",
    "- **Colab**: a file picker will open.  \n",
    "- **Local Jupyter**: place your files in `data/uploads/` and re-run.\n",
    "\n",
    "We extract plain text from PDFs via `PyPDF2`, and read text/markdown files directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ekom1bvHZvS-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "id": "ekom1bvHZvS-",
    "outputId": "92f7d3a1-112c-44f7-91b9-168f30c038b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If on local Jupyter: place at least 3 PDFs/TXT/MD into: data/uploads\n",
      "Colab detected — use the chooser to upload files.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-f4ed7aad-4f30-491e-8726-4029ef24fedf\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-f4ed7aad-4f30-491e-8726-4029ef24fedf\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving annotated-Project%20Title (1).pdf to annotated-Project%20Title (1).pdf\n",
      "Saving mat-report_hurricane-irma_florida.pdf to mat-report_hurricane-irma_florida.pdf\n",
      "Saving NeurIPS-2022-video-diffusion-models-Paper-Conference.pdf to NeurIPS-2022-video-diffusion-models-Paper-Conference.pdf\n",
      "Uploaded: ['annotated-Project%20Title (1).pdf', 'mat-report_hurricane-irma_florida.pdf', 'NeurIPS-2022-video-diffusion-models-Paper-Conference.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, io\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data/uploads\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"If on local Jupyter: place at least 3 PDFs/TXT/MD into:\", DATA_DIR)\n",
    "\n",
    "is_colab = False\n",
    "try:\n",
    "    from google.colab import files as colab_files  # type: ignore\n",
    "    is_colab = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if is_colab:\n",
    "    print(\"Colab detected — use the chooser to upload files.\")\n",
    "    uploaded = colab_files.upload()\n",
    "    for name, data in uploaded.items():\n",
    "        with open(DATA_DIR / name, \"wb\") as f:\n",
    "            f.write(data)\n",
    "    print(\"Uploaded:\", list(uploaded.keys()))\n",
    "else:\n",
    "    print(\"Found:\", [p.name for p in DATA_DIR.glob('*')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "G13Vko-HZvS_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G13Vko-HZvS_",
    "outputId": "c202896b-12a8-4a58-c214-d112dd4fbbba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents.\n",
      "- data/uploads/NeurIPS-2022-video-diffusion-models-Paper-Conference.pdf chars: 46512\n",
      "- data/uploads/annotated-Project%20Title (1).pdf chars: 3616\n",
      "- data/uploads/mat-report_hurricane-irma_florida.pdf chars: 349715\n"
     ]
    }
   ],
   "source": [
    "# Simple loaders: PDF via PyPDF2; TXT/MD by reading text\n",
    "from typing import List, Dict\n",
    "import PyPDF2\n",
    "\n",
    "def load_documents(data_dir: Path) -> List[Dict]:\n",
    "    docs = []\n",
    "    for p in sorted(data_dir.glob(\"*\")):\n",
    "        if p.suffix.lower() == \".pdf\":\n",
    "            try:\n",
    "                text_pages = []\n",
    "                with open(p, \"rb\") as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    for page in reader.pages:\n",
    "                        text_pages.append(page.extract_text() or \"\")\n",
    "                text = \"\\n\".join(text_pages)\n",
    "                docs.append({\"source\": str(p), \"text\": text})\n",
    "            except Exception as e:\n",
    "                print(\"PDF read error:\", p, e)\n",
    "        elif p.suffix.lower() in [\".txt\", \".md\", \".markdown\"]:\n",
    "            try:\n",
    "                text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "                docs.append({\"source\": str(p), \"text\": text})\n",
    "            except Exception as e:\n",
    "                print(\"Text read error:\", p, e)\n",
    "        else:\n",
    "            print(\"Skipping unsupported file:\", p)\n",
    "    return docs\n",
    "\n",
    "docs = load_documents(DATA_DIR)\n",
    "print(f\"Loaded {len(docs)} documents.\")\n",
    "if docs:\n",
    "    for d in docs[:3]:\n",
    "        print('-', d[\"source\"], \"chars:\", len(d[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hQlHsiAXZvS_",
   "metadata": {
    "id": "hQlHsiAXZvS_"
   },
   "source": [
    "## 4) Chunk Documents (default: size=500, overlap=100) and Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xSbEh-H1ZvS_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSbEh-H1ZvS_",
    "outputId": "0ce943ef-03f1-4c11-8ac5-7dbf6943cacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1001\n",
      "First chunk preview (first 400 chars):\n",
      " Video Diffusion Models\n",
      "Jonathan Ho\u0003\n",
      "jonathanho@google.comTim Salimans\u0003\n",
      "salimans@google.comAlexey Gritsenko\n",
      "agritsenko@google.com\n",
      "William Chan\n",
      "williamchan@google.comMohammad Norouzi\n",
      "mnorouzi@google.comDavid J. Fleet\n",
      "davidfleet@google.com\n",
      "Abstract\n",
      "Generating temporally coherent high ﬁdelity video is an important milestone in\n",
      "generative modeling research. We make progress towards this milestone by pr\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Dict, List\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100\n",
    "\n",
    "def chunk_text(text: str, size: int, overlap: int) -> List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + size, len(text))\n",
    "        chunk = text[start:end]\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "        if end == len(text):\n",
    "            break\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for d in docs:\n",
    "    for ch in chunk_text(d[\"text\"], chunk_size, chunk_overlap):\n",
    "        chunks.append({\"source\": d[\"source\"], \"content\": ch})\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "if chunks:\n",
    "    print(\"First chunk preview (first 400 chars):\\n\", chunks[0][\"content\"][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "zUQh31o1ZvTA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUQh31o1ZvTA",
    "outputId": "a2a4cf3f-9ef7-4770-d715-13054d9a9b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved config to rag_gemini_config.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "cfg = {\n",
    "    \"chunk_size\": chunk_size,\n",
    "    \"chunk_overlap\": chunk_overlap,\n",
    "    \"retriever_k\": 4,\n",
    "    \"embedding_model\": \"text-embedding-004\",\n",
    "    \"generation_model\": \"gemini-1.5-flash\"\n",
    "}\n",
    "with open(\"rag_gemini_config.json\",\"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "print(\"Saved config to rag_gemini_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qWbJeNHWZvTA",
   "metadata": {
    "id": "qWbJeNHWZvTA"
   },
   "source": [
    "## 5) Embed Chunks with Gemini (`text-embedding-004`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "N4Mnx4E6ZvTA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "N4Mnx4E6ZvTA",
    "outputId": "67d1b499-6d20-4994-acde-72a2511fc3ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1, 1001, 768)\n",
      "Saved chunk_vectors.npy and chunk_meta.json\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Corrected: Look for GEMINI_API_KEY in environment variables\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Please set GEMINI_API_KEY in your environment.\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-004\"\n",
    "\n",
    "def embed_texts(texts):\n",
    "    # Batch embed for efficiency. The client supports list input.\n",
    "    # Limit very long strings to avoid request size issues.\n",
    "    MAX_LEN = 8000\n",
    "    texts = [t[:MAX_LEN] for t in texts]\n",
    "    resp = genai.embed_content(model=EMBED_MODEL, content=texts)\n",
    "    # Ensure a 2D array is always returned\n",
    "    if isinstance(resp, dict) and \"embedding\" in resp:\n",
    "        return np.array(resp[\"embedding\"], dtype=\"float32\")[None, :] # Keep this for single text case, but handle in retrieve\n",
    "    # For batch: {'embeddings': [{'values': [...]}, ...]}\n",
    "    vals = [np.array(e[\"values\"], dtype=\"float32\") for e in resp.get(\"embeddings\", [])]\n",
    "    return np.stack(vals, axis=0)\n",
    "\n",
    "# Build embeddings for all chunks\n",
    "texts = [c[\"content\"] for c in chunks]\n",
    "if not texts:\n",
    "    raise RuntimeError(\"No chunks to embed. Please add documents in data/uploads/.\")\n",
    "\n",
    "emb_matrix = embed_texts(texts)\n",
    "# Remove the leading dimension if it exists\n",
    "if emb_matrix.ndim == 3 and emb_matrix.shape[0] == 1:\n",
    "    emb_matrix = emb_matrix[0]\n",
    "\n",
    "emb_norms = np.linalg.norm(emb_matrix, axis=1, keepdims=True) + 1e-12\n",
    "emb_matrix_unit = emb_matrix / emb_norms\n",
    "\n",
    "print(\"Embeddings shape:\", emb_matrix_unit.shape)\n",
    "\n",
    "# Persist vectors and metadata for reuse\n",
    "np.save(\"chunk_vectors.npy\", emb_matrix_unit)\n",
    "with open(\"chunk_meta.json\",\"w\") as f:\n",
    "    json.dump(chunks, f, indent=2)\n",
    "\n",
    "print(\"Saved chunk_vectors.npy and chunk_meta.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5gL0pkTbZvTB",
   "metadata": {
    "id": "5gL0pkTbZvTB"
   },
   "source": [
    "## 6) Build Tiny Vector Index & 7) Retriever (cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3AWcRXpGflzV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "3AWcRXpGflzV",
    "outputId": "87da3d70-c3f1-4168-cd44-0fa3974ccb13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS unavailable, falling back to NumPy similarity search. too many values to unpack (expected 2)\n",
      "Top Retrieved Chunks:\n",
      "[1] 0.484 :: data/uploads/NeurIPS-2022-video-diffusion-models-Paper-Conference.pdf :: arting point for further investigation on video diffusion models and investigation into\n",
      "their societal implications, and...\n",
      "[2] 0.462 :: data/uploads/annotated-Project%20Title (1).pdf :: \n",
      "drills,\n",
      " \n",
      "response\n",
      " \n",
      "strategies).\n",
      " \n",
      " \n",
      "The\n",
      " \n",
      "goal\n",
      " \n",
      "is\n",
      " \n",
      "to\n",
      " \n",
      "make\n",
      " \n",
      "disaster\n",
      " \n",
      "preparedness\n",
      " \n",
      "engaging,\n",
      " \n",
      "immersive,\n",
      " \n",
      "...\n",
      "[3] 0.449 :: data/uploads/NeurIPS-2022-video-diffusion-models-Paper-Conference.pdf :: ture hyperparameters,\n",
      "training details, and compute resources are listed in Appendix A.\n",
      "4.1 Unconditional video modeling...\n",
      "[4] 0.449 :: data/uploads/mat-report_hurricane-irma_florida.pdf ::  Technical Publications and Guidance  ......................................................................... 5-9\n",
      "5.7 ...\n",
      "\n",
      "Context ready for Gemini:\n",
      " [Source: data/uploads/NeurIPS-2022-video-diffusion-models-Paper-Conference.pdf]\n",
      "arting point for further investigation on video diffusion models and investigation into\n",
      "their societal implications, and we will aim to explore benchmark evaluations for social and cultural\n",
      "bias in the video generation setting and make the necessary research advances to address them.\n",
      "9\n",
      "References\n",
      "[1]TensorFlow Datasets, a collection of ready-to-use datasets. https://www.tensorflow.org/\n",
      "datasets , 2022.\n",
      "[2]Anurag Arna ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, json\n",
    "\n",
    "# Try FAISS, else fall back to pure NumPy search\n",
    "try:\n",
    "    import faiss\n",
    "    use_faiss = True\n",
    "    dim = emb_matrix_unit.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)  # inner product on unit vectors = cosine similarity\n",
    "    index.add(emb_matrix_unit.astype('float32'))\n",
    "    print(\"FAISS index built with\", index.ntotal, \"vectors.\")\n",
    "except Exception as e:\n",
    "    use_faiss = False\n",
    "    print(\"FAISS unavailable, falling back to NumPy similarity search.\", e)\n",
    "\n",
    "# Load chunk metadata\n",
    "with open(\"chunk_meta.json\") as f:\n",
    "    chunk_meta = json.load(f)\n",
    "\n",
    "def retrieve(query: str, k: int = 4):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most relevant chunks for a query using Gemini embeddings.\n",
    "    Works with FAISS if available, else falls back to NumPy similarity search.\n",
    "    \"\"\"\n",
    "    q_emb = embed_texts([query])\n",
    "\n",
    "    # Normalize query embedding shape\n",
    "    if q_emb.ndim == 3 and q_emb.shape[0] == 1:\n",
    "        q_emb = q_emb[0]\n",
    "    if q_emb.ndim == 2 and q_emb.shape[0] == 1:\n",
    "        q_emb = q_emb[0]\n",
    "\n",
    "    q_emb = q_emb / (np.linalg.norm(q_emb) + 1e-12)\n",
    "\n",
    "    if use_faiss:\n",
    "        q_emb_2d = q_emb[None, :]  # ensure (1,d)\n",
    "        D, I = index.search(q_emb_2d.astype(\"float32\"), k)\n",
    "        idxs = I[0].tolist()\n",
    "        sims = D[0].tolist()\n",
    "    else:\n",
    "        sims = (emb_matrix_unit @ q_emb).ravel().tolist()  # flatten to 1D floats\n",
    "        idxs = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:k]\n",
    "        sims = [sims[i] for i in idxs]\n",
    "\n",
    "    results = []\n",
    "    for rank, (i, s) in enumerate(zip(idxs, sims), start=1):\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": float(s),\n",
    "            \"source\": chunk_meta[i][\"source\"],\n",
    "            \"content\": chunk_meta[i][\"content\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "# Helper: return context string for generation\n",
    "def build_context(results):\n",
    "    return \"\\n\\n\".join(f\"[Source: {r['source']}]\\n{r['content']}\" for r in results)\n",
    "\n",
    "# -------------------------------\n",
    "# Quick sanity check\n",
    "# -------------------------------\n",
    "sample_q = \"Summarize the key datasets and models discussed in these materials.\"\n",
    "hits = retrieve(sample_q, k=4)\n",
    "\n",
    "print(\"Top Retrieved Chunks:\")\n",
    "for h in hits:\n",
    "    print(f\"[{h['rank']}] {h['score']:.3f} :: {h['source']} :: {h['content'][:120]}...\")\n",
    "\n",
    "# Build context string for Gemini generation\n",
    "context_blob = build_context(hits)\n",
    "print(\"\\nContext ready for Gemini:\\n\", context_blob[:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v4NwHMUzZvTB",
   "metadata": {
    "id": "v4NwHMUzZvTB"
   },
   "source": [
    "## 8) Generation with Gemini 1.5 Flash (grounded by retrieved context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "Tcpp7krKZvTB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "Tcpp7krKZvTB",
    "outputId": "6608b17f-dd75-4f62-f57b-4b1f48d54d65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q: What problems does this project aim to solve? List 3–5 key points.\n",
      "A: Florida faces recurring natural disasters (hurricanes, floods, wildfires).  Traditional educational materials lack interactivity and fail to capture the scale and urgency of these events.  The project aims to create engaging disaster preparedness education.  The project will produce subject-oriented disaster education modules that explain scientific causes, show social impacts, and teach civic preparedness.  The goal is to make disaster preparedness engaging.\n",
      "\n",
      "================================================================================\n",
      "Q: Which datasets or data sources are used or proposed?\n",
      "A: The following datasets or data sources are mentioned: TensorFlow Datasets [1], NOAA Hurricane Database – https://www.nhc.noaa.gov/data/, FEMA Disaster Records – https://www.fema.gov/about/reports-and-data/openfema,  wind field maps, wind contour maps, and grids showing flood depths and extents produced by the FEMA Natural Hazard Risk Assessment Program (NHRAP), water surface elevation data compiled from USGS, recorded high water marks, and surge sensor data, data on FEMA Hazard Mitigation Assistance grant projects, and claims from the FEMA National Flood Insurance Program (NFIP).\n",
      "\n",
      "================================================================================\n",
      "Q: What methods, models, or evaluation metrics are mentioned?\n",
      "A: The provided text mentions FVD, FID, and IS as evaluation metrics.  Additionally,  unconditional video generation, conditional video generation (video prediction), and text-conditioned video generation are mentioned as methods or models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "GEN_MODEL = \"gemini-1.5-flash\"\n",
    "generator = genai.GenerativeModel(GEN_MODEL)\n",
    "\n",
    "SYSTEM_INSTRUCTIONS = (\n",
    "    \"You answer ONLY using the provided context. \"\n",
    "    \"If the answer is not clearly supported, say you don't know.\"\n",
    ")\n",
    "\n",
    "def answer_question(question: str, k: int = 4, max_ctx_chars: int = 8000):\n",
    "    ctx_hits = retrieve(question, k=k)\n",
    "    context_blob = \"\\n\\n\".join(\n",
    "        f\"[Source: {h['source']}]\\n{h['content']}\" for h in ctx_hits\n",
    "    )[:max_ctx_chars]\n",
    "    prompt = f\"\"\"{SYSTEM_INSTRUCTIONS}\n",
    "\n",
    "Context:\n",
    "{context_blob}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    resp = generator.generate_content(prompt)\n",
    "    return resp.text, ctx_hits\n",
    "\n",
    "# Ask three domain-specific questions (edit to your project)\n",
    "questions = [\n",
    "    \"What problems does this project aim to solve? List 3–5 key points.\",\n",
    "    \"Which datasets or data sources are used or proposed?\",\n",
    "    \"What methods, models, or evaluation metrics are mentioned?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"=\"*80)\n",
    "    ans, used = answer_question(q, k=4)\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZbjMtEn6ZvTB",
   "metadata": {
    "id": "ZbjMtEn6ZvTB"
   },
   "source": [
    "## 9) Mini-Experiment — Chunk Sensitivity (500/100 vs 300/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8gy8uE50ZvTC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8gy8uE50ZvTC",
    "outputId": "a295cb79-b795-429f-de95-1b5bc887205f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller-chunk count: 1600\n",
      "{\n",
      "  \"query\": \"Summarize project goals and methods.\",\n",
      "  \"baseline\": {\n",
      "    \"chunk_size\": 500,\n",
      "    \"overlap\": 100,\n",
      "    \"top\": [\n",
      "      {\n",
      "        \"rank\": 1,\n",
      "        \"score\": 0.45,\n",
      "        \"source\": \"annotated-Project%20Title (1).pdf\",\n",
      "        \"preview\": \"Project\\n \\nTitle\\n \\nAI-Driven\\n \\n3D\\n \\nVideo\\n \\nGeneration\\n \\nfor\\n \\nMulti-Subject\\n \\nDisaster\\n \\nE\"\n",
      "      },\n",
      "      {\n",
      "        \"rank\": 2,\n",
      "        \"score\": 0.404,\n",
      "        \"source\": \"annotated-Project%20Title (1).pdf\",\n",
      "        \"preview\": \"cts\\n \\n \\nhttps://github.com/firelab/windninja\\n \\nhttps://github.com/huggingface/dif fusers\\n \"\n",
      "      },\n",
      "      {\n",
      "        \"rank\": 3,\n",
      "        \"score\": 0.393,\n",
      "        \"source\": \"NeurIPS-2022-video-diffusion-models-Paper-Conference.pdf\",\n",
      "        \"preview\": \"ts? [N/A]\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n3. If you \"\n",
      "      },\n",
      "      {\n",
      "        \"rank\": 4,\n",
      "        \"score\": 0.392,\n",
      "        \"source\": \"mat-report_hurricane-irma_florida.pdf\",\n",
      "        \"preview\": \"rant projects\\n +Claims from the FEMA National Flood Insurance Program (NFIP) \\n +Data from \"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"smaller\": {\n",
      "    \"chunk_size\": 300,\n",
      "    \"overlap\": 50,\n",
      "    \"top\": [\n",
      "      {\n",
      "        \"rank\": 1,\n",
      "        \"score\": 0.36,\n",
      "        \"source\": \"mat-report_hurricane-irma_florida.pdf\",\n",
      "        \"preview\": \"DATABLE OF CONTENTS1.5.2 Team Composition  ...............................................\"\n",
      "      },\n",
      "      {\n",
      "        \"rank\": 2,\n",
      "        \"score\": 0.346,\n",
      "        \"source\": \"NeurIPS-2022-video-diffusion-models-Paper-Conference.pdf\",\n",
      "        \"preview\": \"(b)Did you describe any potential participant risks, with links to Institutional Review\\nBo\"\n",
      "      },\n",
      "      {\n",
      "        \"rank\": 3,\n",
      "        \"score\": 0.341,\n",
      "        \"source\": \"annotated-Project%20Title (1).pdf\",\n",
      "        \"preview\": \"greSQL\\n \\n \\n\\u25cf\\n \\nVisualization:\\n \\nUnity3D\\n \\n/\\n \\nUnreal\\n \\nEngine(not\\n \\nsure)\\n \\nRelated\\n \\nGitH\"\n",
      "      },\n",
      "      {\n",
      "        \"rank\": 4,\n",
      "        \"score\": 0.341,\n",
      "        \"source\": \"NeurIPS-2022-video-diffusion-models-Paper-Conference.pdf\",\n",
      "        \"preview\": \"ethod in a wider variety of conditioning\\nsettings.\\nOur goal with this work is to advance r\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# Mini-Experiment: Chunk Sensitivity (500/100 vs 300/50)\n",
    "# ---------------------------------------------------\n",
    "import numpy as np, json, os\n",
    "\n",
    "# Helper: build chunks with given size/overlap\n",
    "def build_chunks(docs, size, overlap):\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        for ch in chunk_text(d[\"text\"], size, overlap):\n",
    "            out.append({\"source\": d[\"source\"], \"content\": ch})\n",
    "    return out\n",
    "\n",
    "# Re-build smaller chunks\n",
    "small_chunks = build_chunks(docs, 300, 50)\n",
    "print(\"Smaller-chunk count:\", len(small_chunks))\n",
    "\n",
    "# Embed small chunks\n",
    "small_texts = [c[\"content\"] for c in small_chunks]\n",
    "small_matrix = embed_texts(small_texts)\n",
    "small_matrix = small_matrix / (np.linalg.norm(small_matrix, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "# Generalized retrieval function (works for both baseline & small)\n",
    "def retrieve_matrix(matrix, chunks, query, k=4):\n",
    "    q_emb = embed_texts([query])\n",
    "\n",
    "    # Normalize query embedding shape → always (d,)\n",
    "    if q_emb.ndim == 3 and q_emb.shape[0] == 1:\n",
    "        q_emb = q_emb[0]\n",
    "    if q_emb.ndim == 2 and q_emb.shape[0] == 1:\n",
    "        q_emb = q_emb[0]\n",
    "\n",
    "    q_emb = q_emb / (np.linalg.norm(q_emb) + 1e-12)\n",
    "\n",
    "    sims = (matrix @ q_emb).ravel().tolist()  # flatten to list of floats\n",
    "    idxs = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:k]\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": r + 1,\n",
    "            \"score\": float(sims[i]),\n",
    "            \"source\": chunks[i][\"source\"],\n",
    "            \"content\": chunks[i][\"content\"],\n",
    "        }\n",
    "        for r, i in enumerate(idxs)\n",
    "    ]\n",
    "\n",
    "# Compare baseline vs smaller chunking\n",
    "cmp_q = \"Summarize project goals and methods.\"\n",
    "\n",
    "base_hits = retrieve(cmp_q, k=4)  # baseline retriever you already defined earlier\n",
    "small_hits = retrieve_matrix(small_matrix, small_chunks, cmp_q, k=4)\n",
    "\n",
    "# Summarize results\n",
    "def short(h):\n",
    "    return [\n",
    "        {\n",
    "            \"rank\": x[\"rank\"],\n",
    "            \"score\": round(x[\"score\"], 3),\n",
    "            \"source\": os.path.basename(x[\"source\"]),\n",
    "            \"preview\": x[\"content\"][:90]\n",
    "        }\n",
    "        for x in h\n",
    "    ]\n",
    "\n",
    "cmp = {\n",
    "    \"query\": cmp_q,\n",
    "    \"baseline\": {\"chunk_size\": 500, \"overlap\": 100, \"top\": short(base_hits)},\n",
    "    \"smaller\": {\"chunk_size\": 300, \"overlap\": 50, \"top\": short(small_hits)},\n",
    "}\n",
    "\n",
    "print(json.dumps(cmp, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BWlVCTViZvTC",
   "metadata": {
    "id": "BWlVCTViZvTC"
   },
   "source": [
    "## 10) Save Reproducibility Config → `rag_gemini_config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "RQ9jesbxZvTC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQ9jesbxZvTC",
    "outputId": "3d13aeb7-d30c-47ee-d1c7-48c29a9ff171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final rag_gemini_config.json:\n",
      "{\n",
      "  \"chunk_size\": 500,\n",
      "  \"chunk_overlap\": 100,\n",
      "  \"retriever_k\": 4,\n",
      "  \"embedding_model\": \"text-embedding-004\",\n",
      "  \"generation_model\": \"gemini-1.5-flash\",\n",
      "  \"mini_experiments\": [\n",
      "    {\n",
      "      \"name\": \"chunk_sensitivity\",\n",
      "      \"settings\": [\n",
      "        {\n",
      "          \"size\": 500,\n",
      "          \"overlap\": 100\n",
      "        },\n",
      "        {\n",
      "          \"size\": 300,\n",
      "          \"overlap\": 50\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    cfg = json.load(open(\"rag_gemini_config.json\"))\n",
    "except Exception:\n",
    "    cfg = {}\n",
    "\n",
    "cfg.update({\n",
    "    \"embedding_model\": \"text-embedding-004\",\n",
    "    \"generation_model\": \"gemini-1.5-flash\",\n",
    "    \"retriever_k\": 4,\n",
    "    \"mini_experiments\": [\n",
    "        {\"name\": \"chunk_sensitivity\", \"settings\": [{\"size\":500,\"overlap\":100},{\"size\":300,\"overlap\":50}]}\n",
    "    ]\n",
    "})\n",
    "\n",
    "with open(\"rag_gemini_config.json\", \"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "print(\"Final rag_gemini_config.json:\")\n",
    "print(json.dumps(cfg, indent=2))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
