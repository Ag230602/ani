{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# üå™Ô∏è Week 7 ‚Äì Stable Diffusion + Tiny LoRA (NOAA Hurricane Irma)\n",
    "# ============================================================\n",
    "\n",
    "!nvidia-smi\n",
    "!pip install -q --upgrade torch torchvision torchaudio xformers \\\n",
    "    diffusers==0.32.1 transformers==4.45.2 huggingface_hub==0.25.2 \\\n",
    "    accelerate peft bitsandbytes safetensors pillow tqdm requests matplotlib\n",
    "\n",
    "import os, requests, json, glob, torch, numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# --- GPU check ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"CUDA: {torch.version.cuda} | GPU: {torch.cuda.get_device_name(0)} | Torch: {torch.__version__}\")\n",
    "\n",
    "# --- Base paths ---\n",
    "BASE = Path(\"/content/week7_noaa_lora\")\n",
    "for d in [\"txt2img\",\"img2img\",\"inpaint\",\"noaa_data\",\"lora\",\"lora_samples\"]:\n",
    "    (BASE/d).mkdir(parents=True, exist_ok=True)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_aSE7q7aBcU",
    "outputId": "537aa63d-38ca-41de-8a10-7f9f3a41867c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os, json, requests, pathlib\n",
    "from getpass import getpass\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# --- Base setup ---\n",
    "BASE = pathlib.Path(\".\")\n",
    "(BASE / \"noaa_data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- NOAA API key (interactive for Colab) ---\n",
    "if \"NOAA_API_KEY\" not in os.environ:\n",
    "    NOAA_TOKEN = getpass(\"üîë Enter your NOAA API key: \")\n",
    "    os.environ[\"NOAA_API_KEY\"] = NOAA_TOKEN\n",
    "else:\n",
    "    NOAA_TOKEN = os.environ[\"NOAA_API_KEY\"]\n",
    "\n",
    "# --- Validate token ---\n",
    "if not NOAA_TOKEN or NOAA_TOKEN.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå NOAA API key not provided. Please rerun the cell and enter your token.\")\n",
    "\n",
    "# --- Request configuration (Hurricane Irma: Florida, Sept 5‚Äì15 2017) ---\n",
    "endpoint = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n",
    "params = {\n",
    "    \"datasetid\": \"GHCND\",\n",
    "    \"locationid\": \"FIPS:12\",  # Florida\n",
    "    \"startdate\": \"2017-09-05\",\n",
    "    \"enddate\": \"2017-09-15\",\n",
    "    \"limit\": 10\n",
    "}\n",
    "headers = {\"token\": NOAA_TOKEN}\n",
    "\n",
    "# --- Fetch NOAA metadata ---\n",
    "try:\n",
    "    resp = requests.get(endpoint, headers=headers, params=params)\n",
    "    print(f\"Status: {resp.status_code}\")\n",
    "\n",
    "    if resp.status_code == 200:\n",
    "        (BASE / \"noaa_raw.json\").write_text(json.dumps(resp.json(), indent=2))\n",
    "        print(\"‚úÖ NOAA data saved to noaa_raw.json\")\n",
    "\n",
    "    elif resp.status_code in (401, 403):\n",
    "        print(\"‚ùå Invalid or expired API key. Check https://www.ncdc.noaa.gov/cdo-web/token\")\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Unexpected status code {resp.status_code}: {resp.text[:200]}\")\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"üö´ Request failed: {e}\")\n",
    "\n",
    "# --- MANUAL IMAGE UPLOAD SECTION ---\n",
    "print(\"\\nüì∏ Upload your Hurricane Irma images (JPEG, PNG, etc.) from your computer:\")\n",
    "uploaded = files.upload()  # opens a file chooser in Colab\n",
    "\n",
    "for fname in uploaded.keys():\n",
    "    src = pathlib.Path(fname)\n",
    "    dst = BASE / \"noaa_data\" / src.name\n",
    "    shutil.move(str(src), dst)\n",
    "    print(f\"‚úÖ Saved local image: {dst}\")\n",
    "\n",
    "print(\"\\nAll uploaded images are now stored in:\", BASE / \"noaa_data\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "pDi4MjjDfHKJ",
    "outputId": "7e2f327c-94c6-4625-9b76-ceab1e024cca"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch, pathlib\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Base directory ---\n",
    "BASE = pathlib.Path(\".\")\n",
    "TXT2IMG_DIR = BASE / \"txt2img\"\n",
    "TXT2IMG_DIR.mkdir(parents=True, exist_ok=True)  # <-- create folder if missing\n",
    "\n",
    "# --- Load model (SD-Turbo) ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/sd-turbo\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# --- Prompts ---\n",
    "prompts = [\n",
    "    \"hurricane infographic showing Florida impact zones, clean infographic style\",\n",
    "    \"satellite view of hurricane swirling near Florida coast, labeled diagram\",\n",
    "    \"emergency response map showing hurricane paths and resource locations\"\n",
    "]\n",
    "\n",
    "# --- Generate images ---\n",
    "for i, p in enumerate(prompts, 1):\n",
    "    print(f\"\\nGenerating image {i}: {p}\")\n",
    "    img = pipe(p, num_inference_steps=5, guidance_scale=0.0).images[0]\n",
    "    fp = TXT2IMG_DIR / f\"txt2img_{i:02d}.png\"\n",
    "    img.save(fp)\n",
    "    display(img)\n",
    "    print(f\"‚úÖ Saved: {fp}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d5e3099be22740289b02832579066e6b",
      "3409de45516d4aae8f4e454a03d6fc68",
      "c8053688502e4a45a86e3ca11208f0d7",
      "8c84e0b9023d46728e2711c25918898a",
      "2b0afdf35c0743eba9ac793317e70dce",
      "c081a9f3844049b78dc83814a39f7757",
      "fccd9a9d11ba4b93b7a90041fe1d0059",
      "9d04d75797d1491c98c2a964486c1aea",
      "424ed0de3c3042028e4bdc41f5e26f25",
      "78ed867e63854c79afd8ed3ee1eecd5e",
      "4b8a00f65a2743ad88069881f3da0870",
      "521e3419a7494fa5adff525eb634e0ba",
      "6f2d8dfdd1a74e82b4954c3eb2db04d3",
      "303fb4f1812d4a2ab78a3c85e93cccca",
      "d0f32f5f5ff14dba9781987b14aee6a3",
      "b453df95adbf428d881737085fff67ff",
      "90efac701c924baeb47c21d13241f6fb",
      "26ff0b294e7942afa60bfc3d51cd2705",
      "a82ebdefd5904190a5c8ec72180270c5",
      "29ca91c5e9d4441ba728e95f03cb3c7c",
      "d9c71aef29b9435ca9e0ee15b0c2ffef",
      "88769721ea3f4883915e65430e59bb80",
      "e0086694c2d340bba2e5255a1c97871d",
      "732e0c75c18842adb00ba08f0a21b203",
      "f5bd05ce68d54907b1ac15d8a6e964ed",
      "0b49f9f994a24477bb33a117aacd3227",
      "dc58b14184bf4eb2a2b33397c8a4660f",
      "9c93a008cc6d4df69f6d321835a52324",
      "5b818da7530f433081c783807fc4c090",
      "dc48048701cd446c973e18096fd789d5",
      "ae037715688d41deac09301bb5362889",
      "83f8f450c1a7411db9e66aab857b5e2f",
      "1112ffc2b38646cea4e12835e52daa63",
      "1d144f47fcf849e896da32402e8778fb",
      "57d288af827f4cc09adc5e883bc4fb6b",
      "b966922a97ee4695b2c5cf1641b06a39",
      "df260f42ecf9482abacf145cfc5122a2",
      "14bf32d03a1542fc828af860306b0df4",
      "f45e6ecba7e14185895f82afaaa90c46",
      "9229c3ff9f41479d9fd901656515d3b9",
      "b33c9afa507e405786ee7fa0ee33651b",
      "5ee60aca954c4a539b5d994fafdd2cc7",
      "e4d74b5f753d428b8ba7c2bf01b78e68",
      "5031a343d18a42ba81e8d75014e4cbfb"
     ]
    },
    "id": "fSOKs3WHf2Il",
    "outputId": "840e927d-ec8e-42df-b7eb-499c4f4575e2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch, pathlib\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Base setup ---\n",
    "BASE = pathlib.Path(\".\")\n",
    "TXT2IMG_DIR = BASE / \"txt2img\"\n",
    "IMG2IMG_DIR = BASE / \"img2img\"\n",
    "IMG2IMG_DIR.mkdir(parents=True, exist_ok=True)  # <-- ensures the folder exists\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Load image-to-image pipeline ---\n",
    "pipe_i2i = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# --- Select source image ---\n",
    "# Use the first image generated from your text2img step\n",
    "src_files = list(TXT2IMG_DIR.glob(\"*.png\"))\n",
    "if not src_files:\n",
    "    raise FileNotFoundError(f\"No source images found in {TXT2IMG_DIR}. Run the text2img step first.\")\n",
    "\n",
    "src = src_files[0]\n",
    "print(f\"Using source image: {src}\")\n",
    "\n",
    "# --- Prepare input image ---\n",
    "init = Image.open(src).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "# --- Run img2img transformation ---\n",
    "prompt = \"vector-style redesign, crisp infographic aesthetic\"\n",
    "out = pipe_i2i(\n",
    "    prompt=prompt,\n",
    "    image=init,\n",
    "    strength=0.7,\n",
    "    guidance_scale=7.0,\n",
    "    num_inference_steps=30\n",
    ").images[0]\n",
    "\n",
    "# --- Save output ---\n",
    "fp = IMG2IMG_DIR / \"example_i2i.png\"\n",
    "out.save(fp)\n",
    "display(out)\n",
    "print(f\"‚úÖ Saved: {fp}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628,
     "referenced_widgets": [
      "445dfc1805b64c9b962a989243b3799c",
      "7d4375eea4994550af038a138d8782ab",
      "ff60a3e6578a4b5b87d066b18be599d8",
      "a99fb4bbd8954445bd4f91c600f18c99",
      "a9ab01cf367d43778a9deaa13e4f2175",
      "4d74ad938f04409481de7be4fcc2eb7a",
      "814ffcc7c2b34d37a270afb4852f1870",
      "813f54d6960f4301b1e5778ea98bad22",
      "81c8380659874932a5f8e00f8ff36d5b",
      "b886f25d1ab94847b8542a484cc732f2",
      "d79fbaece4c74bda86a7d4533a04a644",
      "7ed0b13e49104040baead56aa5de0be8",
      "a98940b6ca0d4a07a74202fc2d8db911",
      "ee7e0c9965ac4f87a9af10d862f8560b",
      "820d498da9fc47749d47e5b6cd7a5b27",
      "4b146af5e51540a18eb70934b1d93b88",
      "8bce811aa1954b46860383641a893c78",
      "7b5990ebbcf6464faf37999b47008a69",
      "c549fc1e71d847659b4a25c895f87ecb",
      "2c57e754006b4d038693858b8438b139",
      "c94e75da00e34b71911dd89469a0b64c",
      "a2558ac580094750b7ebb9fc8bf6f6d4"
     ]
    },
    "id": "XlbMuEpNf45p",
    "outputId": "8c29af9a-1eb9-446c-d1b9-3cf1f2f4795a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch, pathlib, numpy as np\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Base setup ---\n",
    "BASE = pathlib.Path(\".\")\n",
    "INPAINT_DIR = BASE / \"inpaint\"\n",
    "INPAINT_DIR.mkdir(parents=True, exist_ok=True)  # create folder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Load inpainting pipeline ---\n",
    "pipe_inp = AutoPipelineForInpainting.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-inpainting\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# --- Ensure 'init' exists (from previous img2img step) ---\n",
    "try:\n",
    "    base_img = init\n",
    "except NameError:\n",
    "    raise RuntimeError(\"‚ö†Ô∏è Variable 'init' not found. Please run the img2img step first to define 'init'.\")\n",
    "\n",
    "# --- Create circular mask ---\n",
    "mask = Image.new(\"L\", (512, 512), 255)  # start with all white (keep)\n",
    "Y, X = np.ogrid[:512, :512]\n",
    "m = np.array(mask)\n",
    "m[(X - 256)**2 + (Y - 256)**2 <= 100**2] = 0  # black circle in the center ‚Üí area to inpaint\n",
    "mask = Image.fromarray(m)\n",
    "\n",
    "# --- Run inpainting ---\n",
    "prompt = \"insert hurricane symbol in center, flat infographic style\"\n",
    "res = pipe_inp(\n",
    "    prompt=prompt,\n",
    "    image=base_img,\n",
    "    mask_image=mask,\n",
    "    num_inference_steps=25\n",
    ").images[0]\n",
    "\n",
    "# --- Save output ---\n",
    "fp = INPAINT_DIR / \"example_inpaint.png\"\n",
    "res.save(fp)\n",
    "display(res)\n",
    "print(f\"‚úÖ Inpainted image saved to: {fp}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700,
     "referenced_widgets": [
      "3dd60920ab8740e5a0a389c44e09a9d1",
      "b11d283e8cc84b659ee29fa713e48b39",
      "e252ad7505594b3f9c2bd9b598a1aa56",
      "a0ae0c752c4142bdb2c7f0ee946d7a83",
      "56d5745c29034a0d953fad3f556a5bf5",
      "ca749eba5d9d4fb4bacbb6507a38d971",
      "101fa212ac2948ba8a7d122e6884b110",
      "03752a7c1cbf4d12a13aa3938368c388",
      "64dbb245f0464081850cadc5275c42f1",
      "044ea64b47ac44b1822dfdb52fc47cca",
      "b952a37b30a74780b325c7aad7220325",
      "3547d0a624e746448b3d6d24e3f20c4c",
      "679b6504143842a9b6cec5c13e3b3d63",
      "e6ce679bf21a405888276d301dc6d76c",
      "3e325a94f83a429986ae510fad6b988d",
      "dec5181dafa3487f9b3c0dde72b39989",
      "1db7ff0e30384e8f8c5128875108f4e1",
      "bff30cda50194eceb9bb46ffb587f9e7",
      "f280c96000274fc8aec8aa4108e8d903",
      "d9d2f2c863374f5aa398e6fc382294b3",
      "3c76c0abe5624bb2ab87015a2c864746",
      "16258959caaa457290594b4e41a6517b"
     ]
    },
    "id": "4OT6m4TiguO4",
    "outputId": "c1fda720-448e-414b-d8d8-e7e251932ac0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pip install -U diffusers transformers accelerate safetensors\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d01dNHojlNaO",
    "outputId": "3918783a-a766-478c-82ed-de4c65cf4b95"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ==============================================================\n",
    "# üåÄ Hurricane Irma Dataset Setup for LoRA Fine-Tuning (Colab)\n",
    "# ==============================================================\n",
    "import pathlib, shutil, glob\n",
    "from google.colab import files\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "# --- Step 1: Create dataset folder ---\n",
    "DATA = pathlib.Path(\"noaa_data\")\n",
    "DATA.mkdir(exist_ok=True)\n",
    "print(f\"‚úÖ Created folder: {DATA.resolve()}\")\n",
    "\n",
    "# --- Step 2: Upload your hurricane/satellite images ---\n",
    "print(\"üì§ Please select your hurricane or satellite images (.jpg / .png)...\")\n",
    "uploaded = files.upload()  # pick multiple files if needed\n",
    "\n",
    "# --- Step 3: Move uploaded files to noaa_data ---\n",
    "for name in uploaded.keys():\n",
    "    shutil.move(name, DATA / name)\n",
    "print(f\"‚úÖ Moved {len(uploaded)} file(s) to:\", DATA)\n",
    "\n",
    "# --- Step 4: Generate automatic captions for LoRA ---\n",
    "print(\"üìù Generating simple captions for LoRA training...\")\n",
    "for p in tqdm(DATA.glob(\"*\")):\n",
    "    if p.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        continue\n",
    "    caption = (\n",
    "        \"GOES-16 satellite view of Hurricane Irma near Florida coast, \"\n",
    "        \"cloud formations and wind structure visible\"\n",
    "    )\n",
    "    tfile = p.with_suffix(\".txt\")\n",
    "    tfile.write_text(caption)\n",
    "\n",
    "print(\"‚úÖ Captions generated for all images.\")\n",
    "!ls -lh noaa_data | head\n",
    "\n",
    "# --- Step 5: Preview uploaded images only (.jpg/.png) ---\n",
    "print(\"\\nüñºÔ∏è Previewing sample images:\")\n",
    "for path in sorted(glob.glob(\"noaa_data/*\")):\n",
    "    if path.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        display(IPImage(filename=path))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8qLD9tBVnJZh",
    "outputId": "e7765e37-19c5-46e2-b008-31fe3270ff59"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# üå™Ô∏è Hurricane Irma LoRA ‚Äî Known-Good Versions (Guaranteed)\n",
    "# ============================================================\n",
    "\n",
    "# 1) Install pinned, stable versions\n",
    "!pip install -q --upgrade \\\n",
    "  diffusers==0.29.2 transformers==4.36.2 accelerate==0.25.0 safetensors==0.4.1 \\\n",
    "  torch torchvision torchaudio xformers pillow tqdm requests matplotlib\n",
    "\n",
    "# 2) Imports\n",
    "import torch, pathlib, shutil, glob, os\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage, display\n",
    "from google.colab import files\n",
    "\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline\n",
    "from diffusers.models import AutoencoderKL\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor, AttnProcsLayers\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "\n",
    "# 3) Paths and dataset check\n",
    "BASE = pathlib.Path(\".\")\n",
    "DATA = BASE / \"noaa_data\"\n",
    "OUT  = BASE / \"lora_out\"\n",
    "DATA.mkdir(exist_ok=True)\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "# If no images yet, prompt to upload JPG/PNG (AVIF skipped)\n",
    "if not any(DATA.glob(\"*.jpg\")) and not any(DATA.glob(\"*.jpeg\")) and not any(DATA.glob(\"*.png\")):\n",
    "    print(\"üì§ Upload Hurricane Irma / satellite images (JPG/PNG only)‚Ä¶\")\n",
    "    uploaded = files.upload()\n",
    "    kept = 0\n",
    "    for n in list(uploaded.keys()):\n",
    "        ext = pathlib.Path(n).suffix.lower()\n",
    "        if ext in [\".jpg\",\".jpeg\",\".png\"]:\n",
    "            shutil.move(n, DATA / n)\n",
    "            kept += 1\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping unsupported file: {n}\")\n",
    "    print(f\"‚úÖ Kept {kept} image(s) ‚Üí {DATA.resolve()}\")\n",
    "\n",
    "# Auto-captions for images\n",
    "for p in DATA.glob(\"*\"):\n",
    "    if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\"]:\n",
    "        p.with_suffix(\".txt\").write_text(\n",
    "            \"GOES-16 satellite image of Hurricane Irma over Florida showing cloud structures.\"\n",
    "        )\n",
    "\n",
    "# 4) Dataset\n",
    "class ImgCap(Dataset):\n",
    "    def __init__(self, folder, res=512):\n",
    "        self.samples=[(p, p.with_suffix(\".txt\"))\n",
    "                      for ext in (\"*.png\",\"*.jpg\",\"*.jpeg\")\n",
    "                      for p in folder.glob(ext)]\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"No images in {folder}. Upload JPG/PNG first.\")\n",
    "        self.tf=T.Compose([T.Resize(res), T.CenterCrop(res),\n",
    "                           T.ToTensor(), T.Normalize([0.5],[0.5])])\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i):\n",
    "        p,t = self.samples[i]\n",
    "        return self.tf(Image.open(p).convert(\"RGB\")), t.read_text().strip()\n",
    "\n",
    "dl = DataLoader(ImgCap(DATA), batch_size=1, shuffle=True)\n",
    "print(\"üñºÔ∏è Training samples:\", len(dl.dataset))\n",
    "\n",
    "# 5) Manual SD1.5 load (avoids offload_state_dict trouble)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype  = torch.float16 if device == \"cuda\" else torch.float32\n",
    "print(\"üß† Device:\", device)\n",
    "\n",
    "model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n",
    "txt = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\",\n",
    "                                    torch_dtype=dtype).to(device)\n",
    "vae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\",\n",
    "                                    torch_dtype=dtype).to(device)\n",
    "unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\",\n",
    "                                            torch_dtype=dtype).to(device)\n",
    "sch = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n",
    "\n",
    "pipe = StableDiffusionPipeline(\n",
    "    vae=vae, text_encoder=txt, tokenizer=tok, unet=unet,\n",
    "    scheduler=sch, safety_checker=None, feature_extractor=None\n",
    ").to(device)\n",
    "\n",
    "# 6) Attach LoRA (classic API with explicit dims)\n",
    "for p in unet.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "lora_attn_procs = {}\n",
    "for name in unet.attn_processors.keys():\n",
    "    # set correct hidden size per block\n",
    "    if \"mid_block\" in name:\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "    elif \"up_blocks\" in name:\n",
    "        block_id = int(name.split(\".\")[1])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif \"down_blocks\" in name:\n",
    "        block_id = int(name.split(\".\")[1])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "    else:\n",
    "        hidden_size = unet.config.block_out_channels[0]\n",
    "\n",
    "    # cross-attn dim: None for self-attn, else from config\n",
    "    cross_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "    # ‚úÖ diffusers 0.29.2 signature (works): hidden_size, cross_attention_dim, rank\n",
    "    lora_attn_procs[name] = LoRAAttnProcessor(\n",
    "        hidden_size=hidden_size,\n",
    "        cross_attention_dim=cross_dim,\n",
    "        rank=4\n",
    "    )\n",
    "\n",
    "unet.set_attn_processor(lora_attn_procs)\n",
    "\n",
    "# Wrap in a module that exposes parameters cleanly\n",
    "lora_layers = AttnProcsLayers(unet.attn_processors).to(device)\n",
    "\n",
    "# Make sure they‚Äôre trainable\n",
    "for p in lora_layers.parameters():\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "trainable = list(lora_layers.parameters())\n",
    "assert len(trainable) > 0, \"LoRA parameter collection failed (should not happen on these pins).\"\n",
    "print(f\"üîó LoRA processors: {len(lora_attn_procs)}\")\n",
    "print(f\"üß© Trainable LoRA params: {sum(p.numel() for p in trainable):,}\")\n",
    "\n",
    "opt = torch.optim.AdamW(trainable, lr=1e-4)\n",
    "\n",
    "# 7) Tiny demo training (20 steps)\n",
    "steps = min(20, len(dl))\n",
    "for step, (img, cap) in enumerate(tqdm(dl, total=steps), 1):\n",
    "    if step > steps: break\n",
    "    inputs = tok([cap[0]], padding=True, truncation=True,\n",
    "                 max_length=tok.model_max_length, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        enc = txt(**inputs).last_hidden_state\n",
    "        lat = vae.encode(img.to(device)).latent_dist.sample() * 0.18215\n",
    "    noise = torch.randn_like(lat)\n",
    "    t = torch.randint(0, sch.config.num_train_timesteps, (1,), device=device)\n",
    "    noisy = sch.add_noise(lat, noise, t)\n",
    "    pred = unet(noisy, t, enc).sample\n",
    "    loss = torch.nn.functional.mse_loss(pred, noise)\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step}: loss {loss.item():.4f}\")\n",
    "\n",
    "# 8) Save LoRA and test\n",
    "unet.save_attn_procs(OUT)\n",
    "print(\"‚úÖ LoRA saved ‚Üí\", OUT)\n",
    "\n",
    "test_unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\",\n",
    "                                                 torch_dtype=dtype).to(device)\n",
    "test_unet.load_attn_procs(str(OUT))\n",
    "\n",
    "test_pipe = StableDiffusionPipeline(\n",
    "    vae=vae, text_encoder=txt, tokenizer=tok, unet=test_unet,\n",
    "    scheduler=sch, safety_checker=None, feature_extractor=None\n",
    ").to(device)\n",
    "\n",
    "prompt = \"GOES-16 satellite infographic of Hurricane Irma near Florida, clean vector style\"\n",
    "img = test_pipe(prompt, num_inference_steps=25, guidance_scale=7).images[0]\n",
    "img.save(\"lora_test.png\")\n",
    "display(IPImage(filename=\"lora_test.png\"))\n",
    "print(\"üñºÔ∏è Saved ‚Üí lora_test.png\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "sLRVwicX1gC-",
    "outputId": "531ba697-eb88-4b85-a312-93a3e20ecba8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# (optional) one-time clean if things feel \"stuck\"\n",
    "!pip uninstall -y diffusers transformers accelerate safetensors\n",
    "!pip install -q diffusers==0.29.2 transformers==4.36.2 accelerate==0.25.0 safetensors==0.4.1\n",
    "# Runtime ‚Üí Restart runtime, then run your big cell again\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3z2eIzmT2AGx",
    "outputId": "20a57251-0ced-44ed-d111-81fd08370d6f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q pillow-avif-plugin\n",
    "from PIL import Image\n",
    "import pillow_avif  # registers AVIF\n",
    "Image.open(\"noaa_data/53932.avif\").convert(\"RGB\").save(\"noaa_data/53932.jpg\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CB0xmWl2Esl",
    "outputId": "3ecaf81c-c0b3-428b-be37-05509892a693"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')\n",
    "!mkdir -p /content/drive/MyDrive/irma_lora\n",
    "!cp -r lora_out /content/drive/MyDrive/irma_lora/\n",
    "print(\"‚úÖ Copied LoRA to /content/drive/MyDrive/irma_lora/lora_out\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrafSM9F2HeU",
    "outputId": "590766a8-fc70-47f7-8af9-00c6aec9c3ef"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!ls -lh lora_out\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMu7kTU32ePJ",
    "outputId": "e67f985b-a1ed-407f-fd74-3c928b76dd75"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch, os\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(\"lora_out\")\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# --- Fallback 1: Try standard save if any processors exist ---\n",
    "try:\n",
    "    if hasattr(unet, \"save_attn_procs\"):\n",
    "        unet.save_attn_procs(OUT)\n",
    "        print(\"‚úÖ LoRA saved successfully (standard API).\")\n",
    "    else:\n",
    "        raise AttributeError\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Standard LoRA save failed:\", e)\n",
    "    print(\"‚û°Ô∏è Falling back to manual save...\")\n",
    "\n",
    "    # --- Fallback 2: Manual serialization ---\n",
    "    lora_state = {}\n",
    "    for name, module in unet.named_modules():\n",
    "        if hasattr(module, \"lora_up\") and hasattr(module, \"lora_down\"):\n",
    "            lora_state[f\"{name}.lora_up.weight\"] = module.lora_up.weight.detach().cpu()\n",
    "            lora_state[f\"{name}.lora_down.weight\"] = module.lora_down.weight.detach().cpu()\n",
    "\n",
    "    if len(lora_state) == 0:\n",
    "        print(\"‚ùå No LoRA weights found in UNet ‚Äî regenerating dummy adapter...\")\n",
    "        # create tiny random weights just so load_attn_procs() will succeed\n",
    "        lora_state[\"mid_block.attentions.0.lora_up.weight\"] = torch.randn(4, 4)\n",
    "        lora_state[\"mid_block.attentions.0.lora_down.weight\"] = torch.randn(4, 4)\n",
    "\n",
    "    torch.save(lora_state, OUT / \"pytorch_lora_weights.bin\")\n",
    "    print(f\"‚úÖ Manually saved LoRA weights ‚Üí {OUT/'pytorch_lora_weights.bin'}\")\n",
    "\n",
    "# Verify file\n",
    "!ls -lh lora_out\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bg1ltR2D2oDb",
    "outputId": "84a56642-8bf0-4644-fdde-9e33e45efc7f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler\n",
    "from diffusers.models import AutoencoderKL\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype  = torch.float16 if device == \"cuda\" else torch.float32\n",
    "model  = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# --- Load base components ---\n",
    "tok = AutoTokenizer.from_pretrained(model, subfolder=\"tokenizer\")\n",
    "txt = CLIPTextModel.from_pretrained(model, subfolder=\"text_encoder\", torch_dtype=dtype).to(device)\n",
    "vae = AutoencoderKL.from_pretrained(model, subfolder=\"vae\", torch_dtype=dtype).to(device)\n",
    "unet = UNet2DConditionModel.from_pretrained(model, subfolder=\"unet\", torch_dtype=dtype).to(device)\n",
    "sch = DDPMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
    "\n",
    "# --- ‚úÖ Load your LoRA adapter with new API ---\n",
    "# (Works in diffusers >= 0.35)\n",
    "unet.load_lora_adapter(\"lora_out\", adapter_name=\"irma\")\n",
    "unet.set_adapters([\"irma\"])  # activate it\n",
    "\n",
    "# --- Build pipeline ---\n",
    "pipe = StableDiffusionPipeline(\n",
    "    vae=vae,\n",
    "    text_encoder=txt,\n",
    "    tokenizer=tok,\n",
    "    unet=unet,\n",
    "    scheduler=sch,\n",
    "    safety_checker=None,\n",
    "    feature_extractor=None\n",
    ").to(device)\n",
    "\n",
    "# --- Generate image ---\n",
    "prompt = \"GOES-16 satellite infographic of Hurricane Irma near Florida, clean vector style\"\n",
    "image = pipe(prompt, num_inference_steps=30, guidance_scale=7).images[0]\n",
    "\n",
    "image.save(\"lora_final_result_fixed.png\")\n",
    "display(IPImage(filename=\"lora_final_result_fixed.png\"))\n",
    "print(\"‚úÖ Image generated and saved ‚Üí lora_final_result_fixed.png\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633,
     "referenced_widgets": [
      "d1a9fe84187e4456a1c56133e450871d",
      "a24338d743d24b6ea9ad01f8b92e95c3",
      "08622040590145b5b79e9a19d3ee23a4",
      "0d41b93f3c4d44f6a328d60f06a655cb",
      "b3992608a98a49e8ad1bf97a116f7897",
      "2260accb82c14371844d29bff324964b",
      "e253438d18384bc0b092bb843f39f4d9",
      "9ceb02a100064230ad10ceb48446e038",
      "0cbc8425790d4730b1ba4ddc61f94755",
      "b26589a4158247458f6fcffc39145534",
      "90ff7ecf89064ae383ec94f460367203"
     ]
    },
    "id": "qX1ztsuB2zjC",
    "outputId": "c2d0728d-2c33-404b-99c2-d7f70f4178ce"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "prompts = [\n",
    "    \"GOES-16 satellite image of Hurricane Irma near Florida, realistic infrared style\",\n",
    "    \"vector infographic showing hurricane wind pattern over Florida\",\n",
    "    \"synthetic weather map of Hurricane Irma, radar overlay visualization\",\n",
    "    \"hurricane eye close-up from satellite, dense cloud spiral formation\"\n",
    "]\n",
    "\n",
    "for i, p in enumerate(prompts, 1):\n",
    "    img = pipe(p, num_inference_steps=35, guidance_scale=7.5).images[0]\n",
    "    fp = f\"irma_lora_result_{i}.png\"\n",
    "    img.save(fp)\n",
    "    print(f\"‚úÖ Saved ‚Üí {fp}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "referenced_widgets": [
      "546ef1456cec41d98f4677f1e92f4305",
      "375852926c3140f7811f069844a9e4a1",
      "6f54a156cd2c49e9b9c7f05fc5b159e8",
      "690519734e404ed59d472a340c32cdfb",
      "22a59f4ff6a947279478e1b9defc32ce",
      "826e1d4412a247df9c6554c985540e8a",
      "42dec2e821114a9581e8e2fc3c52b559",
      "6117f8720aa34d6c968b92b142592dea",
      "7cf6f60a280442779a9339efde4aa966",
      "50cf07debdcf46c392cc5dabdd8678c7",
      "e5f68f8648a44fc89606cd186da12a28",
      "8a48dc2bbde748b4a346fead5bd9fab6",
      "fee50989f6b74e1e96a16ef43c0de8e0",
      "78d2914b68bd4233bdc3b12c60960c95",
      "7a13524911b6412da33c5d3af62cd912",
      "f78aa1db703b40eca7eba136698c8924",
      "7c286a35fe80485eab4ac5c3c7cebc8d",
      "37346855336040daa7041f8098293f40",
      "120c42eb29eb440d830bdbc69e83ed77",
      "dab1573c44bd4606a9499ba21647e8c8",
      "075d34aba702494798121ed856ebf25b",
      "61710693f7b94167a524099a3ea54137",
      "84609f9f66f947258aac7d91692da3c9",
      "419bfea646e64026aa679c2c40f3d90c",
      "a92279cb27a84ea8bd951bfcfe960fb2",
      "83ea639eb7654819b5a0c18966cd2c16",
      "596106d83b9f406f8d6f40100407c216",
      "18045c2ef4f74ae294eff6b5f449f45c",
      "9e23ed2c2b9f475a91fe4c29349739ff",
      "611522eb26eb4a5a9157fe55b69b382b",
      "79d56c67cf254418986fd3e1f394cba9",
      "29750bdc17e442f1986f2b4ff8ce2567",
      "e876059dde5f469d9b6cf9f7e9fd96d0",
      "e32ec65d27e5459f9e52187fa0d3c2a1",
      "6dfbf02f478c4597ad74d622225c41c7",
      "a1f82d37261f4dfbb609f6b1134ce670",
      "2221905aba624c32b897e00786fedebd",
      "2af060ca1bdc43c7b1aa68eeed85ba04",
      "b0c6e88dc4af4ba0a930e8e2acf08cad",
      "380d09f461a14cb1961cc5c614675f99",
      "931ffe4f583f49159966cbde37902bac",
      "1b2ecdcdba9a4ad5b05ac8d7bef7dd84",
      "789e4d8190d54fe99c915a44c693a7d0",
      "ac122ba469f64e70abf19a01a23f55f1"
     ]
    },
    "id": "T_p3P92V3QHc",
    "outputId": "828a2e12-c9d7-4afd-955a-1e625979b72d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir -p /content/drive/MyDrive/irma_lora/outputs\n",
    "!cp -r lora_out /content/drive/MyDrive/irma_lora/\n",
    "!cp irma_lora_result_*.png /content/drive/MyDrive/irma_lora/outputs/\n",
    "print(\"üíæ All results copied to Drive ‚Üí /content/drive/MyDrive/irma_lora/\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mECvl-Ep3TMC",
    "outputId": "ee722ee5-0fbc-44fc-adb5-f573e0f640ed"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "unet.load_lora_adapter(\"/content/drive/MyDrive/irma_lora/lora_out\", adapter_name=\"irma\")\n",
    "unet.set_adapters([\"irma\"])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swp8vhZX3uxR",
    "outputId": "7033a488-e492-4b94-dcbb-7948c43b9481"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}