{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145afbec",
   "metadata": {},
   "source": [
    "\n",
    "# Florida Case Study — Hurricane Irma (2017): Forecasting Agent Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Install & Imports (Colab/Local)\n",
    "# If running locally and these are already installed, you may skip.\n",
    "!pip -q install pandas numpy matplotlib scikit-learn torch torchvision torchaudio statsmodels requests pytz --upgrade\n",
    "\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import pytz\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4a6d1",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Data Ingestion\n",
    "\n",
    "We try two sources (either should work; use whichever responds).  \n",
    "- **IBTrACS v4** CSV (global archive) — filter to *IRMA (2017)* in the North Atlantic.  \n",
    "- **NHC HURDAT2** Best Track text — parse AL112017 (Irma).\n",
    "\n",
    "> If one URL fails due to maintenance, run the other cell or paste a working mirror.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Define helper: robust HTTP GET (with retries)\n",
    "def http_get(url, max_tries=3, timeout=60):\n",
    "    last_e = None\n",
    "    for i in range(max_tries):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=timeout)\n",
    "            if r.status_code == 200:\n",
    "                return r\n",
    "            last_e = RuntimeError(f\"HTTP {r.status_code} for {url}\")\n",
    "        except Exception as e:\n",
    "            last_e = e\n",
    "        time.sleep(2*(i+1))\n",
    "    if last_e:\n",
    "        raise last_e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Option A: IBTrACS CSV (Filter for Irma 2017 in North Atlantic)\n",
    "# IBTrACS v04 CSV (global list). If this path changes, search \"ibtracs v04 csv ALL list\" and update.\n",
    "IBTRACS_URL = \"https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/provisional/csv/ibtracs.ALL.list.v04r00.csv\"\n",
    "\n",
    "try:\n",
    "    r = http_get(IBTRACS_URL)\n",
    "    df = pd.read_csv(io.StringIO(r.text))\n",
    "    # Common IBTrACS columns (may vary slightly by release):\n",
    "    # 'NAME', 'SEASON', 'BASIN', 'ISO_TIME', 'LAT', 'LON', 'USA_SSHS', 'USA_WIND', 'USA_PRES', etc.\n",
    "    # Filter Irma 2017 in North Atlantic (NA)\n",
    "    df_irma = df[\n",
    "        (df['NAME'].str.upper() == 'IRMA') & \n",
    "        (df['SEASON'] == 2017) & \n",
    "        (df['BASIN'].str.upper().str.contains('NA'))\n",
    "    ].copy()\n",
    "\n",
    "    # Fallback if 'BASIN' label differs\n",
    "    if df_irma.empty and 'SUBBASIN' in df.columns:\n",
    "        df_irma = df[\n",
    "            (df['NAME'].str.upper() == 'IRMA') & \n",
    "            (df['SEASON'] == 2017) & \n",
    "            (df['SUBBASIN'].str.upper().str.contains('NA'))\n",
    "        ].copy()\n",
    "\n",
    "    if df_irma.empty:\n",
    "        raise ValueError(\"Could not filter Irma 2017 in NA from IBTrACS. Try the HURDAT2 option below.\")\n",
    "\n",
    "    # Clean & coerce\n",
    "    # Normalize time\n",
    "    df_irma['ISO_TIME'] = pd.to_datetime(df_irma['ISO_TIME'], errors='coerce', utc=True)\n",
    "    # Use USA_WIND and USA_PRES if available, else WIND/PRES\n",
    "    wind_col = 'USA_WIND' if 'USA_WIND' in df_irma.columns else 'WIND'\n",
    "    pres_col = 'USA_PRES' if 'USA_PRES' in df_irma.columns else 'PRES'\n",
    "\n",
    "    df_irma = df_irma[['ISO_TIME','LAT','LON',wind_col, pres_col]].rename(\n",
    "        columns={wind_col:'WIND_KT', pres_col:'PRES_MB'}\n",
    "    )\n",
    "    df_irma = df_irma.dropna(subset=['ISO_TIME','LAT','LON'])\n",
    "    df_irma = df_irma.sort_values('ISO_TIME').reset_index(drop=True)\n",
    "\n",
    "    source_used = \"IBTrACS\"\n",
    "    print(f\"Loaded {len(df_irma)} Irma records from IBTrACS.\")\n",
    "except Exception as e:\n",
    "    print(\"IBTrACS failed with:\", e)\n",
    "    df_irma = None\n",
    "    source_used = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a79aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Option B: NHC HURDAT2 (parse AL112017 Irma)\n",
    "# If IBTrACS failed above, try HURDAT2.\n",
    "# This URL may be updated yearly; if it 404s, search \"hurdat2 atlantic text file NHC\" and update.\n",
    "HURDAT_URL = \"https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2023-120523.txt\"\n",
    "\n",
    "if df_irma is None:\n",
    "    try:\n",
    "        txt = http_get(HURDAT_URL).text\n",
    "        # Find AL112017 — Irma block\n",
    "        lines = txt.splitlines()\n",
    "        start_idx = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"AL, 11, 2017\" in line and \"IRMA\" in line.upper():\n",
    "                start_idx = i\n",
    "                break\n",
    "        if start_idx is None:\n",
    "            raise ValueError(\"Could not find AL112017 Irma header in HURDAT2 file.\")\n",
    "\n",
    "        # Header line format like:\n",
    "        # AL, 11, 2017, IRMA, ... number of entries\n",
    "        header = lines[start_idx]\n",
    "        # next N lines are the best-track entries; count comes after header commas\n",
    "        # e.g., \"AL, 11, 2017, IRMA, 137, ...\"\n",
    "        parts = [p.strip() for p in header.split(\",\")]\n",
    "        n_entries = int(parts[4])\n",
    "\n",
    "        rows = []\n",
    "        for j in range(1, n_entries+1):\n",
    "            L = lines[start_idx + j]\n",
    "            # Format documented by NHC: date,time,record id,status,lat,lon,max wind,min pres,...\n",
    "            # Example: 20170901, 0000, , TD,  17.0N,  33.0W,  25, 1007, ...\n",
    "            vals = [v.strip() for v in L.split(\",\")]\n",
    "            dts = datetime.strptime(vals[0] + vals[1], \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n",
    "            lat = float(vals[4][:-1]) * (1 if vals[4].endswith(\"N\") else -1)\n",
    "            lon = float(vals[5][:-1]) * (-1 if vals[5].endswith(\"W\") else 1)\n",
    "            wind = float(vals[6]) if vals[6] != \"\" else np.nan\n",
    "            pres = float(vals[7]) if vals[7] != \"\" else np.nan\n",
    "            rows.append((dts, lat, lon, wind, pres))\n",
    "        df_irma = pd.DataFrame(rows, columns=[\"ISO_TIME\",\"LAT\",\"LON\",\"WIND_KT\",\"PRES_MB\"]).sort_values(\"ISO_TIME\").reset_index(drop=True)\n",
    "        source_used = \"HURDAT2\"\n",
    "        print(f\"Loaded {len(df_irma)} Irma records from HURDAT2.\")\n",
    "    except Exception as e:\n",
    "        print(\"HURDAT2 failed with:\", e)\n",
    "\n",
    "if df_irma is None:\n",
    "    raise SystemExit(\"No data loaded from either source. Please update the URLs and rerun.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10206d11",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Preprocess & Feature Engineering\n",
    "\n",
    "We align timestamps, compute deltas (speed/bearing proxies), and build features for short‑term forecasting:\n",
    "- Inputs `X`: \n",
    "\n",
    "  `LAT_t, LON_t, WIND_t, PRES_t, ΔLAT_t, ΔLON_t, hour_sin, hour_cos, day_sin, day_cos`  \n",
    "- Targets `y`: next‑step `(LAT_{t+1}, LON_{t+1}, WIND_{t+1})`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639cc174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df_irma.copy()\n",
    "df = df.dropna(subset=[\"LAT\",\"LON\"]).copy()\n",
    "df[\"WIND_KT\"] = df[\"WIND_KT\"].astype(float)\n",
    "df[\"PRES_MB\"] = df[\"PRES_MB\"].astype(float)\n",
    "\n",
    "# Ensure uniform 3-hour steps (many best-track datasets are 6-hour; we can forward-fill if needed)\n",
    "df = df.set_index(\"ISO_TIME\").sort_index()\n",
    "df = df.resample(\"3H\").interpolate(limit_direction=\"both\")\n",
    "df = df.reset_index()\n",
    "\n",
    "# Deltas (approx)\n",
    "df[\"DLAT\"] = df[\"LAT\"].diff().fillna(0.0)\n",
    "df[\"DLON\"] = df[\"LON\"].diff().fillna(0.0)\n",
    "\n",
    "# Cyclical time features\n",
    "t = df[\"ISO_TIME\"].dt.tz_convert(\"UTC\") if df[\"ISO_TIME\"].dt.tz is not None else df[\"ISO_TIME\"].dt.tz_localize(\"UTC\")\n",
    "hour = t.dt.hour.values\n",
    "day_of_year = t.dt.dayofyear.values\n",
    "\n",
    "df[\"HOUR_SIN\"] = np.sin(2*np.pi*hour/24.0)\n",
    "df[\"HOUR_COS\"] = np.cos(2*np.pi*hour/24.0)\n",
    "df[\"DOY_SIN\"]  = np.sin(2*np.pi*day_of_year/365.0)\n",
    "df[\"DOY_COS\"]  = np.cos(2*np.pi*day_of_year/365.0)\n",
    "\n",
    "# Targets: next-step\n",
    "df[\"LAT_next\"]  = df[\"LAT\"].shift(-1)\n",
    "df[\"LON_next\"]  = df[\"LON\"].shift(-1)\n",
    "df[\"WIND_next\"] = df[\"WIND_KT\"].shift(-1)\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207a872",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Train/Test Split & Dataset Loader\n",
    "\n",
    "We use the first 80% for training and last 20% for testing (time-ordered).  \n",
    "Sequence length can be tuned via `SEQ_LEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa50e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEATURES = [\"LAT\",\"LON\",\"WIND_KT\",\"PRES_MB\",\"DLAT\",\"DLON\",\"HOUR_SIN\",\"HOUR_COS\",\"DOY_SIN\",\"DOY_COS\"]\n",
    "TARGETS  = [\"LAT_next\",\"LON_next\",\"WIND_next\"]\n",
    "\n",
    "all_X = df[FEATURES].values.astype(np.float32)\n",
    "all_y = df[TARGETS].values.astype(np.float32)\n",
    "\n",
    "n = len(df)\n",
    "n_train = int(0.8 * n)\n",
    "train_X, test_X = all_X[:n_train], all_X[n_train:]\n",
    "train_y, test_y = all_y[:n_train], all_y[n_train:]\n",
    "\n",
    "# Scale features (targets in native units for interpretability)\n",
    "scaler = StandardScaler()\n",
    "train_X_sc = scaler.fit_transform(train_X)\n",
    "test_X_sc  = scaler.transform(test_X)\n",
    "\n",
    "SEQ_LEN = 6  # use past 6 steps (~18 hours with 3h interval)\n",
    "\n",
    "def make_sequences(X, y, seq_len):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-seq_len):\n",
    "        Xs.append(X[i:i+seq_len])\n",
    "        ys.append(y[i+seq_len])\n",
    "    return np.stack(Xs), np.stack(ys)\n",
    "\n",
    "Xtr_seq, ytr_seq = make_sequences(train_X_sc, train_y, SEQ_LEN)\n",
    "Xte_seq, yte_seq = make_sequences(test_X_sc,  test_y,  SEQ_LEN)\n",
    "\n",
    "print(\"Train seq:\", Xtr_seq.shape, \"Test seq:\", Xte_seq.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74ec73",
   "metadata": {},
   "source": [
    "\n",
    "## 4) LSTM Forecasting Model\n",
    "\n",
    "A small, fast **LSTM** predicts next‑step `(lat, lon, wind)` from the last `SEQ_LEN` windows of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=1, output_dim=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n",
    "        self.fc   = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]  # (B, H)\n",
    "        return self.fc(last)\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SeqDataset(Xtr_seq, ytr_seq)\n",
    "test_ds  = SeqDataset(Xte_seq, yte_seq)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMForecast(input_dim=Xtr_seq.shape[-1], hidden_dim=96, num_layers=1, output_dim=3).to(device)\n",
    "criterion = nn.L1Loss()  # MAE\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Train the model\n",
    "EPOCHS = 50\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item() * len(xb)\n",
    "    train_loss = loss_sum / len(train_ds)\n",
    "\n",
    "    # quick val\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_sum = 0.0\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            val_sum += loss.item() * len(xb)\n",
    "        val_loss = val_sum / len(test_ds)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | train MAE {train_loss:.4f} | val MAE {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4a475",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Evaluation vs Persistence Baseline\n",
    "\n",
    "- **Model**: next‑step MAE for latitude, longitude, and wind.  \n",
    "- **Baseline (Persistence)**: predict next = current (`LAT_t → LAT_{t+1}`, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_test = []\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        pred = model(xb).cpu().numpy()\n",
    "        pred_test.append(pred)\n",
    "    pred_test = np.vstack(pred_test)\n",
    "\n",
    "# Align to original unwindowed test targets:\n",
    "y_true = yte_seq\n",
    "\n",
    "mae_lat = mean_absolute_error(y_true[:,0], pred_test[:,0])\n",
    "mae_lon = mean_absolute_error(y_true[:,1], pred_test[:,1])\n",
    "mae_wnd = mean_absolute_error(y_true[:,2], pred_test[:,2])\n",
    "\n",
    "print(f\"Model MAE — LAT: {mae_lat:.3f}°, LON: {mae_lon:.3f}°, WIND: {mae_wnd:.3f} kt\")\n",
    "\n",
    "# Persistence baseline on the same alignment: use the last step in the input sequence\n",
    "persist = Xte_seq[:, -1, :]\n",
    "# map indices in FEATURES to lat/lon/wind positions\n",
    "i_lat = FEATURES.index(\"LAT\")\n",
    "i_lon = FEATURES.index(\"LON\")\n",
    "i_wnd = FEATURES.index(\"WIND_KT\")\n",
    "\n",
    "persist_lat = persist[:, i_lat]\n",
    "persist_lon = persist[:, i_lon]\n",
    "persist_wnd = persist[:, i_wnd]\n",
    "\n",
    "b_mae_lat = mean_absolute_error(y_true[:,0], persist_lat)\n",
    "b_mae_lon = mean_absolute_error(y_true[:,1], persist_lon)\n",
    "b_mae_wnd = mean_absolute_error(y_true[:,2], persist_wnd)\n",
    "\n",
    "print(f\"Baseline (Persistence) MAE — LAT: {b_mae_lat:.3f}°, LON: {b_mae_lon:.3f}°, WIND: {b_mae_wnd:.3f} kt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4ea44",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Visualize Track & Wind Forecasts\n",
    "\n",
    "We plot **actual vs predicted** for the test window (time‑aligned to predictions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a time index matching predictions in test set\n",
    "test_times = df[\"ISO_TIME\"].iloc[len(df) - len(test_X) + SEQ_LEN : len(df)]  # windowed alignment\n",
    "test_times = test_times.reset_index(drop=True)[:len(pred_test)]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(y_true[:,1], y_true[:,0], marker='o', linewidth=1, label=\"Actual Track\")\n",
    "plt.plot(pred_test[:,1], pred_test[:,0], marker='x', linewidth=1, label=\"Predicted Track\")\n",
    "plt.xlabel(\"Longitude (°)\")\n",
    "plt.ylabel(\"Latitude (°)\")\n",
    "plt.title(\"Hurricane Irma — Track (Test Period)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(test_times, y_true[:,2], label=\"Actual Wind (kt)\")\n",
    "plt.plot(test_times, pred_test[:,2], label=\"Predicted Wind (kt)\")\n",
    "plt.xlabel(\"Time (UTC)\")\n",
    "plt.ylabel(\"Max Sustained Wind (kt)\")\n",
    "plt.title(\"Hurricane Irma — Wind Forecast (Test Period)\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37545d93",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (Optional) ARIMA Baseline for Wind Only\n",
    "\n",
    "A simple univariate ARIMA on wind (best‑track series) for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51950a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit ARIMA(p,d,q) on training wind; forecast next steps to align with test length\n",
    "train_wind = train_y[:,2]  # WIND_next\n",
    "# We'll model the current wind (one‑step shifted): align with simplicity\n",
    "wind_series = df[\"WIND_KT\"].values.astype(float)\n",
    "split_idx = len(train_X)  # train/test split in raw index\n",
    "wind_train = wind_series[:split_idx]\n",
    "wind_test  = wind_series[split_idx:]\n",
    "\n",
    "# small ARIMA; if it fails to converge, tweak order\n",
    "try:\n",
    "    arima = ARIMA(wind_train, order=(2,1,2)).fit()\n",
    "    fc = arima.forecast(steps=len(wind_test))\n",
    "    mae_arima = mean_absolute_error(wind_test, fc)\n",
    "    print(f\"ARIMA Wind-only MAE (kt): {mae_arima:.3f}\")\n",
    "except Exception as e:\n",
    "    print(\"ARIMA failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcbd75",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Save/Load Trained Model\n",
    "\n",
    "Save a minimal checkpoint (state_dict + feature list + scaler stats) for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CKPT_PATH = \"irma_lstm_checkpoint.pt\"\n",
    "meta = {\n",
    "    \"features\": FEATURES,\n",
    "    \"seq_len\": SEQ_LEN,\n",
    "    \"scaler_mean\": scaler.mean_.tolist(),\n",
    "    \"scaler_scale\": scaler.scale_.tolist(),\n",
    "    \"source_used\": source_used,\n",
    "}\n",
    "torch.save({\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"meta\": meta,\n",
    "}, CKPT_PATH)\n",
    "\n",
    "print(\"Saved:\", CKPT_PATH, \"| source:\", source_used)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606fa74",
   "metadata": {},
   "source": [
    "\n",
    "## 9) (Optional) FastAPI Serving Stub\n",
    "\n",
    "Use this snippet in a separate `server.py` to expose a `/predict` endpoint.  \n",
    "It expects the **last `SEQ_LEN` frames of features** matching `FEATURES`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api_stub = r'''\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "FEATURES = {features}\n",
    "SEQ_LEN = {seq_len}\n",
    "\n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=96, num_layers=1, output_dim=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n",
    "        self.fc   = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.fc(last)\n",
    "\n",
    "class Window(BaseModel):\n",
    "    x: list  # shape = [SEQ_LEN, len(FEATURES)]\n",
    "\n",
    "def load_model(ckpt_path=\"irma_lstm_checkpoint.pt\"):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    meta = ckpt[\"meta\"]\n",
    "    model = LSTMForecast(input_dim=len(meta[\"features\"]))\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model.eval()\n",
    "    return model, meta\n",
    "\n",
    "app = FastAPI()\n",
    "model, meta = load_model()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(window: Window):\n",
    "    X = np.array(window.x, dtype=np.float32)\n",
    "    if X.shape != (SEQ_LEN, len(FEATURES)):\n",
    "        return {\"error\": f\"Expected shape ({SEQ_LEN}, {len(FEATURES)})\"}\n",
    "    # Standardize using saved scaler stats\n",
    "    mean = np.array(meta[\"scaler_mean\"], dtype=np.float32)\n",
    "    scale = np.array(meta[\"scaler_scale\"], dtype=np.float32)\n",
    "    Xs = (X - mean) / scale\n",
    "    xb = torch.tensor(Xs, dtype=torch.float32).unsqueeze(0)  # (1, T, F)\n",
    "    with torch.no_grad():\n",
    "        pred = model(xb).numpy().tolist()[0]\n",
    "    return {\"lat_next\": pred[0], \"lon_next\": pred[1], \"wind_next_kt\": pred[2]}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''.format(features=json.dumps(FEATURES), seq_len=SEQ_LEN)\n",
    "\n",
    "with open(\"fastapi_stub_server.py\", \"w\") as f:\n",
    "    f.write(api_stub)\n",
    "\n",
    "print(\"Wrote fastapi_stub_server.py\")\n",
    "print(\"\\nRun locally:\")\n",
    "print(\"  uvicorn fastapi_stub_server:app --reload --port 8000\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
