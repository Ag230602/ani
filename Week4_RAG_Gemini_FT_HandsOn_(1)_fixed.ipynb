{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "GGEuNBUFml_9",
   "metadata": {
    "id": "GGEuNBUFml_9"
   },
   "source": [
    "# CS5588 ‚Äî Week 4: RAG + Gemini + Fine-Tuning (Hands-On)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CzAmwFc5ml_-",
   "metadata": {
    "id": "CzAmwFc5ml_-"
   },
   "source": [
    "## 1) Install & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "x97EJbvQml_-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x97EJbvQml_-",
    "outputId": "1917b1da-221d-472e-940d-f0a306fc94ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing: ['google-generativeai>=0.7.2', 'PyPDF2>=3.0.1', 'numpy>=1.23.0']\n",
      "Installing: ['faiss-cpu>=1.8.0']\n",
      "Installing: ['transformers>=4.44.2', 'accelerate>=0.34.0', 'peft>=0.11.1', 'datasets>=2.21.0']\n",
      "‚úÖ Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "def pip_install(pkgs):\n",
    "    print(\"Installing:\", pkgs)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs, check=True)\n",
    "\n",
    "pip_install([\"google-generativeai>=0.7.2\",\"PyPDF2>=3.0.1\",\"numpy>=1.23.0\"])\n",
    "try:\n",
    "    pip_install([\"faiss-cpu>=1.8.0\"])\n",
    "except Exception as e:\n",
    "    print(\"FAISS skipped:\", e)\n",
    "try:\n",
    "    pip_install([\"transformers>=4.44.2\",\"accelerate>=0.34.0\",\"peft>=0.11.1\",\"datasets>=2.21.0\"])\n",
    "except Exception as e:\n",
    "    print(\"PEFT stack skipped:\", e)\n",
    "print(\"‚úÖ Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zBtFkV6iml__",
   "metadata": {
    "id": "zBtFkV6iml__"
   },
   "source": [
    "## 2) Log Environment ‚Üí env_rag.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "kEWNHngGml__",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEWNHngGml__",
    "outputId": "1bf5311a-7e4a-485b-869f-fdd2c425dc29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2310805173.py:4: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  env={\"timestamp_utc\": datetime.datetime.utcnow().isoformat()+\"Z\",\"python\": platform.python_version(),\"platform\": platform.platform()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"timestamp_utc\": \"2025-09-18T20:55:08.000807Z\",\n",
      "  \"python\": \"3.12.11\",\n",
      "  \"platform\": \"Linux-6.1.123+-x86_64-with-glibc2.35\",\n",
      "  \"google-generativeai\": \"0.8.5\",\n",
      "  \"numpy\": \"2.0.2\",\n",
      "  \"PyPDF2\": \"3.0.1\",\n",
      "  \"faiss\": \"available\",\n",
      "  \"torch\": \"2.8.0+cu126\",\n",
      "  \"cuda_available\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json, platform, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "env={\"timestamp_utc\": datetime.datetime.utcnow().isoformat()+\"Z\",\"python\": platform.python_version(),\"platform\": platform.platform()}\n",
    "try:\n",
    "    import google.generativeai as genai; env[\"google-generativeai\"]=getattr(genai,\"__version__\",\"unknown\")\n",
    "except Exception as e: env[\"google-generativeai\"]=f\"unavailable ({e})\"\n",
    "try:\n",
    "    import numpy as np; env[\"numpy\"]=np.__version__\n",
    "except Exception as e: env[\"numpy\"]=f\"unavailable ({e})\"\n",
    "try:\n",
    "    import PyPDF2; env[\"PyPDF2\"]=PyPDF2.__version__\n",
    "except Exception as e: env[\"PyPDF2\"]=f\"unavailable ({e})\"\n",
    "try:\n",
    "    import faiss; env[\"faiss\"]=\"available\"\n",
    "except Exception: env[\"faiss\"]=\"unavailable\"\n",
    "try:\n",
    "    import torch; env[\"torch\"]=torch.__version__; env[\"cuda_available\"]=bool(torch.cuda.is_available())\n",
    "except Exception: env[\"torch\"]=\"N/A\"\n",
    "\n",
    "Path(\"runs\").mkdir(exist_ok=True)\n",
    "with open(\"env_rag.json\",\"w\") as f: json.dump(env,f,indent=2)\n",
    "print(json.dumps(env,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zPu123rJml__",
   "metadata": {
    "id": "zPu123rJml__"
   },
   "source": [
    "## 3) Load Documents (PDF/TXT/MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "SXcdN4vkml__",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "id": "SXcdN4vkml__",
    "outputId": "41caf332-ce15-4b47-b263-b0f22048448b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Jupyter: put at least 3 PDFs/TXT/MD into data/uploads\n",
      "Colab: upload now\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-8f56ea76-d833-4594-9a07-077c34595613\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-8f56ea76-d833-4594-9a07-077c34595613\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving AL112017_Irma.pdf to AL112017_Irma.pdf\n",
      "Saving mat-report_hurricane-irma_florida.pdf to mat-report_hurricane-irma_florida (1).pdf\n",
      "Saving annotated-Project%20Title (1).pdf to annotated-Project%20Title (1) (1).pdf\n",
      "Uploaded: ['AL112017_Irma.pdf', 'mat-report_hurricane-irma_florida (1).pdf', 'annotated-Project%20Title (1) (1).pdf']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, io\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR=Path(\"data/uploads\"); DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Local Jupyter: put at least 3 PDFs/TXT/MD into\", DATA_DIR)\n",
    "\n",
    "is_colab=False\n",
    "try:\n",
    "    from google.colab import files as colab_files\n",
    "    is_colab=True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if is_colab:\n",
    "    print(\"Colab: upload now\")\n",
    "    uploaded=colab_files.upload()\n",
    "    for name,data in uploaded.items():\n",
    "        with open(DATA_DIR/name,\"wb\") as f: f.write(data)\n",
    "    print(\"Uploaded:\", list(uploaded.keys()))\n",
    "else:\n",
    "    print(\"Found:\", [p.name for p in DATA_DIR.glob('*')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "StfVBVlLmmAA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StfVBVlLmmAA",
    "outputId": "df16302a-ac0a-4469-d682-be4d3284096b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 docs\n",
      "- data/uploads/AL112017_Irma.pdf chars: 163316\n",
      "- data/uploads/annotated-Project%20Title (1) (1).pdf chars: 3616\n",
      "- data/uploads/annotated-Project%20Title (1).pdf chars: 3616\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import PyPDF2\n",
    "\n",
    "def load_documents(data_dir: Path) -> List[Dict]:\n",
    "    docs=[]\n",
    "    for p in sorted(data_dir.glob(\"*\")):\n",
    "        if p.suffix.lower()==\".pdf\":\n",
    "            try:\n",
    "                text_pages=[]\n",
    "                with open(p,\"rb\") as f:\n",
    "                    reader=PyPDF2.PdfReader(f)\n",
    "                    for page in reader.pages:\n",
    "                        text_pages.append(page.extract_text() or \"\")\n",
    "                text=\"\\n\".join(text_pages)\n",
    "                docs.append({\"source\": str(p), \"text\": text})\n",
    "            except Exception as e:\n",
    "                print(\"PDF read error:\", p, e)\n",
    "        elif p.suffix.lower() in [\".txt\",\".md\",\".markdown\"]:\n",
    "            try:\n",
    "                text=p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "                docs.append({\"source\": str(p), \"text\": text})\n",
    "            except Exception as e:\n",
    "                print(\"Text read error:\", p, e)\n",
    "        else:\n",
    "            print(\"Skipping:\", p)\n",
    "    return docs\n",
    "\n",
    "docs=load_documents(DATA_DIR)\n",
    "print(\"Loaded\", len(docs), \"docs\")\n",
    "if docs:\n",
    "    for d in docs[:3]: print(\"-\", d[\"source\"], \"chars:\", len(d[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zbry-0-QmmAA",
   "metadata": {
    "id": "zbry-0-QmmAA"
   },
   "source": [
    "## 4) Chunk Documents (size=500, overlap=100) and Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "m3YANW8AmmAA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3YANW8AmmAA",
    "outputId": "f9e20168-08bc-4519-d269-c4b17310bdd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 2177\n",
      "First chunk preview:\n",
      "  NATIONAL HURRICANE CENTER \n",
      "TROPICAL CYCLONE REPORT  \n",
      " \n",
      " HURRICANE IRMA  \n",
      " (AL112017)  \n",
      " 30 August‚Äì12 September 2017  \n",
      " John P. Cangialosi, Andrew S. Latto, and Robbie Berg \n",
      "National Hurricane Center \n",
      "24 September 20211 \n",
      " \n",
      "VIIRS SATELLITE IMAGE OF HURRICANE IRMA WHEN IT WAS AT ITS PEAK INTENSITY AND MADE LANDFALL ON BARBUDA AT 0535 \n",
      "UTC 6 SEPTEMBER. \n",
      "Irma was a long-lived Cape Verde hurricane that\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "chunk_size=500; chunk_overlap=100\n",
    "\n",
    "def chunk_text(text:str, size:int, overlap:int)->List[str]:\n",
    "    chunks=[]; start=0\n",
    "    while start<len(text):\n",
    "        end=min(start+size, len(text))\n",
    "        piece=text[start:end]\n",
    "        if piece.strip(): chunks.append(piece)\n",
    "        if end==len(text): break\n",
    "        start=end-overlap\n",
    "        if start<0: start=0\n",
    "    return chunks\n",
    "\n",
    "chunks=[]\n",
    "for d in docs:\n",
    "    for ch in chunk_text(d[\"text\"], chunk_size, chunk_overlap):\n",
    "        chunks.append({\"source\": d[\"source\"], \"content\": ch})\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "if chunks: print(\"First chunk preview:\\n\", chunks[0][\"content\"][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9SShImHLmmAA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SShImHLmmAA",
    "outputId": "07caeb2a-c29a-4676-859c-668fa4145998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rag_gemini_ft_config.json\n"
     ]
    }
   ],
   "source": [
    "# Save initial config\n",
    "import json\n",
    "cfg={\"chunk_size\":chunk_size,\"chunk_overlap\":chunk_overlap,\"retriever_k\":4,\"embedding_model\":\"text-embedding-004\",\"generation_model\":\"gemini-1.5-flash\"}\n",
    "with open(\"rag_gemini_ft_config.json\",\"w\") as f: json.dump(cfg,f,indent=2)\n",
    "print(\"Saved rag_gemini_ft_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ciQ5Cb3QmmAA",
   "metadata": {
    "id": "ciQ5Cb3QmmAA"
   },
   "source": [
    "## 5) Embed with Gemini (text-embedding-004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeakqYlsmmAA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "aeakqYlsmmAA",
    "outputId": "229ce9c3-d42a-496a-a1a6-0cd51726cfea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1, 2177, 768)\n",
      "Saved chunk_vectors.npy and chunk_meta.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, google.generativeai as genai\n",
    "\n",
    "# Set API key first\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"0000000000000000000000000000\"\n",
    "\n",
    "\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key: raise RuntimeError(\"Please set GEMINI_API_KEY in your environment.\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "EMBED_MODEL=\"text-embedding-004\"\n",
    "\n",
    "def embed_texts(texts):\n",
    "    MAX_LEN=8000\n",
    "    texts=[t[:MAX_LEN] for t in texts]\n",
    "    resp=genai.embed_content(model=EMBED_MODEL, content=texts)\n",
    "    if isinstance(resp, dict) and \"embedding\" in resp:\n",
    "        return np.array(resp[\"embedding\"], dtype=\"float32\")[None,:]\n",
    "    vals=[np.array(e[\"values\"], dtype=\"float32\") for e in resp.get(\"embeddings\",[])]\n",
    "    return np.stack(vals, axis=0)\n",
    "\n",
    "texts=[c[\"content\"] for c in chunks]\n",
    "if not texts: raise RuntimeError(\"No chunks to embed. Add docs in data/uploads/.\")\n",
    "emb_matrix=embed_texts(texts)\n",
    "emb_matrix_unit=emb_matrix/(np.linalg.norm(emb_matrix,axis=1,keepdims=True)+1e-12)\n",
    "print(\"Embeddings shape:\", emb_matrix_unit.shape)\n",
    "\n",
    "np.save(\"chunk_vectors.npy\", emb_matrix_unit)\n",
    "with open(\"chunk_meta.json\",\"w\") as f: json.dump(chunks,f,indent=2)\n",
    "print(\"Saved chunk_vectors.npy and chunk_meta.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nqXldmdDmmAA",
   "metadata": {
    "id": "nqXldmdDmmAA"
   },
   "source": [
    "## 6) Vector Index & Retriever (FAISS or NumPy cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "N78FqqqjqgRa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N78FqqqjqgRa",
    "outputId": "50d98ca0-79ee-45ec-d039-a8ab2ff0d5ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu --no-cache-dir --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "K2nPktGnp9Sa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "K2nPktGnp9Sa",
    "outputId": "00b7364a-e553-4ec3-e935-c8d0e8ebe5a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è FAISS unavailable, falling back to NumPy. Reason: too many values to unpack (expected 2)\n",
      "[1] 0.322 :: data/uploads/annotated-Project%20Title (1) (1).pdf :: \n",
      "drills,\n",
      " \n",
      "response\n",
      " \n",
      "strategies).\n",
      " \n",
      " \n",
      "The\n",
      " \n",
      "goal\n",
      " \n",
      "is\n",
      " \n",
      "to\n",
      " \n",
      "make\n",
      " \n",
      "disaster\n",
      " \n",
      "preparedness\n",
      " \n",
      "engag...\n",
      "[2] 0.322 :: data/uploads/annotated-Project%20Title (1).pdf :: \n",
      "drills,\n",
      " \n",
      "response\n",
      " \n",
      "strategies).\n",
      " \n",
      " \n",
      "The\n",
      " \n",
      "goal\n",
      " \n",
      "is\n",
      " \n",
      "to\n",
      " \n",
      "make\n",
      " \n",
      "disaster\n",
      " \n",
      "preparedness\n",
      " \n",
      "engag...\n",
      "[3] 0.320 :: data/uploads/annotated-Project%20Title (1) (1).pdf :: cts\n",
      " \n",
      " \n",
      "https://github.com/firelab/windninja\n",
      " \n",
      "https://github.com/huggingface/dif fusers\n",
      " \n",
      "https://g...\n",
      "[4] 0.320 :: data/uploads/annotated-Project%20Title (1).pdf :: cts\n",
      " \n",
      " \n",
      "https://github.com/firelab/windninja\n",
      " \n",
      "https://github.com/huggingface/dif fusers\n",
      " \n",
      "https://g...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# Step 0: Install FAISS (run once in Colab/Jupyter)\n",
    "# -------------------------------------------------\n",
    "# In Colab:\n",
    "# !pip install faiss-cpu --no-cache-dir --upgrade\n",
    "#\n",
    "# In Conda (local Jupyter):\n",
    "# conda install -c conda-forge faiss-cpu -y\n",
    "\n",
    "import numpy as np, json\n",
    "\n",
    "# -----------------------------\n",
    "# Try FAISS setup\n",
    "# -----------------------------\n",
    "use_faiss = False\n",
    "index = None\n",
    "try:\n",
    "    import faiss\n",
    "    dim = emb_matrix_unit.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)  # inner product = cosine sim for normalized vecs\n",
    "    index.add(emb_matrix_unit.astype(\"float32\"))\n",
    "    use_faiss = True\n",
    "    print(\"‚úÖ FAISS index built with\", index.ntotal, \"vectors.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è FAISS unavailable, falling back to NumPy. Reason:\", e)\n",
    "    use_faiss = False\n",
    "\n",
    "# -----------------------------\n",
    "# Load metadata\n",
    "# -----------------------------\n",
    "with open(\"chunk_meta.json\") as f:\n",
    "    chunk_meta = json.load(f)\n",
    "\n",
    "# -----------------------------\n",
    "# Retrieval function\n",
    "# -----------------------------\n",
    "# def retrieve(query: str, k: int = 4):\n",
    "#     q_emb = embed_texts([query])\n",
    "\n",
    "#     # Always force shape (d,)\n",
    "#     if q_emb.ndim > 1:\n",
    "#         q_emb = q_emb.squeeze(0)\n",
    "#     q_emb = q_emb / (np.linalg.norm(q_emb) + 1e-12)\n",
    "\n",
    "#     if use_faiss:\n",
    "#         q2d = q_emb[None, :].astype(\"float32\")\n",
    "#         D, I = index.search(q2d, k)\n",
    "#         idxs, sims = I[0].tolist(), D[0].tolist()\n",
    "#     else:\n",
    "#         sims = (emb_matrix_unit @ q_emb).ravel().tolist()\n",
    "#         idxs = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:k]\n",
    "#         sims = [sims[i] for i in idxs]\n",
    "\n",
    "#     results = []\n",
    "#     for rank, (i, s) in enumerate(zip(idxs, sims), start=1):\n",
    "#         results.append({\n",
    "#             \"rank\": rank,\n",
    "#             \"score\": float(s),\n",
    "#             \"source\": chunk_meta[i][\"source\"],\n",
    "#             \"content\": chunk_meta[i][\"content\"]\n",
    "#         })\n",
    "#     return results\n",
    "def retrieve(query: str, k: int = 4):\n",
    "    q_emb = embed_texts([query])\n",
    "\n",
    "    # Ensure q_emb is always shape (d,)\n",
    "    q_emb = np.array(q_emb).squeeze()\n",
    "    if q_emb.ndim != 1:\n",
    "        raise ValueError(f\"Query embedding has wrong shape: {q_emb.shape}\")\n",
    "\n",
    "    # Normalize\n",
    "    q_emb = q_emb / (np.linalg.norm(q_emb) + 1e-12)\n",
    "\n",
    "    if use_faiss:\n",
    "        q2d = q_emb[None, :].astype(\"float32\")   # shape (1, d)\n",
    "        D, I = index.search(q2d, k)\n",
    "        idxs = I[0].tolist()\n",
    "        sims = D[0].tolist()\n",
    "    else:\n",
    "        sims = (emb_matrix_unit @ q_emb).ravel().tolist()  # flatten to 1D\n",
    "        idxs = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:k]\n",
    "        sims = [float(sims[i]) for i in idxs]  # force float conversion\n",
    "\n",
    "    results = []\n",
    "    for rank, (i, s) in enumerate(zip(idxs, sims), start=1):\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": float(s),  # guaranteed scalar\n",
    "            \"source\": chunk_meta[i][\"source\"],\n",
    "            \"content\": chunk_meta[i][\"content\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Build context string\n",
    "# -----------------------------\n",
    "def build_context(results):\n",
    "    return \"\\n\\n\".join(f\"[Source: {r['source']}]\\n{r['content']}\" for r in results)\n",
    "\n",
    "# -----------------------------\n",
    "# üîç Quick test\n",
    "# -----------------------------\n",
    "hits = retrieve(\"Summarize key datasets and models.\", k=4)\n",
    "for h in hits:\n",
    "    print(f\"[{h['rank']}] {h['score']:.3f} :: {h['source']} :: {h['content'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r5f06ZOYmmAB",
   "metadata": {
    "id": "r5f06ZOYmmAB"
   },
   "source": [
    "## 7) Generation with Gemini 1.5 Flash (grounded by retrieved context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dumcpllWmmAB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "dumcpllWmmAB",
    "outputId": "affcd15e-0c43-4dad-cc13-2a187d09e7e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q: What problems does this project aim to solve? \n",
      "A: Florida faces recurring natural disasters such as hurricanes, floods, and wildfires. Traditional educational materials are limited in interactivity and fail to capture the scale and urgency of such events.\n",
      "\n",
      "================================================================================\n",
      "Q: Which datasets are proposed? \n",
      "A: The NOAA Hurricane Database (https://www.nhc.noaa.gov/data/) and FEMA Disaster Records (https://www.fema.gov/about/reports-and-data/openfema) are the proposed datasets.\n",
      "\n",
      "================================================================================\n",
      "Q: What methods or metrics are mentioned? \n",
      "A: Advanced Dvorak Technique (ADT), Advanced Microwave Sounding Unit (AMSU), NASA Global Precipitation Mission (GPM), European Space Agency‚Äôs Advanced Scatterometer (ASCAT), and Defense Meteorological Satellite Program (DMSP) satellites are mentioned.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "GEN_MODEL=\"gemini-1.5-flash\"\n",
    "generator=genai.GenerativeModel(GEN_MODEL)\n",
    "\n",
    "SYS=(\"You answer ONLY using the provided context. \"\n",
    "     \"If the answer is not clearly supported, say you don't know.\")\n",
    "\n",
    "def answer_question(question:str, k:int=4, max_ctx_chars:int=8000):\n",
    "    ctx_hits=retrieve(question, k=k)\n",
    "    context_blob=\"\\n\\n\".join(f\"[Source: {h['source']}]\\n{h['content']}\" for h in ctx_hits)[:max_ctx_chars]\n",
    "    prompt=f\"\"\"{SYS}\n",
    "\n",
    "Context:\n",
    "{context_blob}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    resp=generator.generate_content(prompt)\n",
    "    return resp.text, ctx_hits\n",
    "\n",
    "for q in [\"What problems does this project aim to solve?\",\n",
    "          \"Which datasets are proposed?\",\n",
    "          \"What methods or metrics are mentioned?\"]:\n",
    "    print(\"=\"*80); a,_=answer_question(q, k=4); print(\"Q:\", q, \"\\nA:\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3IyEThQHmmAB",
   "metadata": {
    "id": "3IyEThQHmmAB"
   },
   "source": [
    "## 8) Fine-Tuning Options (A: Simulated QA Memory, B: LoRA/PEFT optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WADS--WDmmAB",
   "metadata": {
    "id": "WADS--WDmmAB"
   },
   "source": [
    "### 8A) Simulated Fine-Tuning (QA Memory + RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3X2gnCI2mmAB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "3X2gnCI2mmAB",
    "outputId": "6ecec1a3-9691-4348-9c59-3048bd32716d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved qa_bank.jsonl with 3 pairs.\n",
      "=== Base RAG vs 'Tuned' (QA memory) ===\n",
      "\n",
      "Q: Summarize the project goals and expected outcomes. \n",
      "--- Base ---\n",
      " The project aims to build a Generative AI-powered 3D video generation system producing subject-oriented disaster education modules.  These videos will explain scientific causes (meteorology, geography), show social impacts (evacuation, relief operations), and teach civic preparedness (safety drills, response strategies).  The goal is to make disaster preparedness engaging.\n",
      " \n",
      "--- Tuned ---\n",
      " The project aims to build a Generative AI-powered 3D video generation system for multi-subject disaster education, specifically focusing on Florida's hurricanes, floods, and wildfires.  The generated videos will explain the scientific causes (meteorology, geography), show social impacts (evacuation, relief operations), and teach civic preparedness (safety drills, response strategies).  The overall goal is to create engaging and interactive educational modules to improve disaster preparedness.\n",
      "\n",
      "\n",
      "Q: What risks or limitations are mentioned and mitigations? \n",
      "--- Base ---\n",
      " The provided text mentions that conclusions or recommendations do not necessarily reflect the views of FEMA.  Neither FEMA nor its employees make any warranty or assume liability for the accuracy, completeness, or usefulness of information in the publication. Users assume all liability arising from use of the information.  No mitigations are mentioned.\n",
      " \n",
      "--- Tuned ---\n",
      " The provided text mentions limitations related to the accuracy and completeness of information from FEMA publications.  The disclaimer states that FEMA does not warrant the accuracy, completeness, or usefulness of any information, product, or process included and users assume all liability arising from using this information.  No specific risks or mitigations beyond this disclaimer are mentioned in the provided context.  Further, the context lacks information about the project's specific methods and data handling, preventing a full risk assessment.  The QA memory indicates that the project use\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "seed_questions=[\n",
    "    \"Give a concise project summary (objectives, data sources, methods).\",\n",
    "    \"List the key datasets and why they are relevant.\",\n",
    "    \"Explain the main modeling approach and any fine-tuning strategy.\"\n",
    "]\n",
    "\n",
    "qa_bank=[]\n",
    "for q in seed_questions:\n",
    "    try:\n",
    "        a,_=answer_question(q, k=4)\n",
    "    except Exception as e:\n",
    "        a=f\"[Error: {e}]\"\n",
    "    qa_bank.append({\"question\": q, \"answer\": a})\n",
    "\n",
    "with open(\"qa_bank.jsonl\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for row in qa_bank: f.write(json.dumps(row, ensure_ascii=False)+\"\\n\")\n",
    "print(\"Saved qa_bank.jsonl with\", len(qa_bank), \"pairs.\")\n",
    "\n",
    "def make_memory(qa_pairs, max_chars=4000):\n",
    "    blob=\"\\n\\n\".join([f\"Q: {x['question']}\\nA: {x['answer']}\" for x in qa_pairs])\n",
    "    return blob[:max_chars]\n",
    "memory_blob=make_memory(qa_bank)\n",
    "\n",
    "def answer_with_memory(question:str, k:int=4):\n",
    "    ctx_hits=retrieve(question, k=k)\n",
    "    context_blob=\"\\n\\n\".join(f\"[Source: {h['source']}]\\n{h['content']}\" for h in ctx_hits)[:8000]\n",
    "    prompt=f\"\"\"You are an expert assistant for this specific project domain.\n",
    "Use BOTH the QA memory and the retrieved context to answer.\n",
    "Prefer facts in context; if unclear, rely on QA memory.\n",
    "\n",
    "QA Memory:\n",
    "{memory_blob}\n",
    "\n",
    "Context:\n",
    "{context_blob}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    resp=generator.generate_content(prompt)\n",
    "    return resp.text, ctx_hits\n",
    "\n",
    "print(\"=== Base RAG vs 'Tuned' (QA memory) ===\")\n",
    "for q in [\"Summarize the project goals and expected outcomes.\",\n",
    "          \"What risks or limitations are mentioned and mitigations?\"]:\n",
    "    a_base,_=answer_question(q,k=4)\n",
    "    a_tuned,_=answer_with_memory(q,k=4)\n",
    "    print(\"\\nQ:\", q, \"\\n--- Base ---\\n\", a_base[:600], \"\\n--- Tuned ---\\n\", a_tuned[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YHAGJZtgmmAB",
   "metadata": {
    "id": "YHAGJZtgmmAB"
   },
   "source": [
    "### 8B) LoRA/PEFT on a Small Open-Source Model (Optional; GPU recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rtGQQKAJmmAB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405,
     "referenced_widgets": [
      "e888aaf65e214d2a8224134334be64fc",
      "845195f7db2a4d54925695ba93625384",
      "5cfdf2517fb74dbc9410468cb6390a5d",
      "2bb809e2c3bc4098baeef88792f4ea89",
      "4a9d76a3e0e144a2a4b381632020eff3",
      "811ebd119ae24913a14a7931af20eae0",
      "4a2a5d098ef64142bad4e6ea1f62223d",
      "dbadff008d6146b680a88be8a48c1de0",
      "24d67c8019a34baeaef9b5441645a67a",
      "9bdc083d4bb24649b3805eee6c5eb17a",
      "5f53cf4cb2664f7b87e4848296b5fe2d"
     ]
    },
    "id": "rtGQQKAJmmAB",
    "outputId": "4d8ee285-e061-4312-a852-33531b5d561c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e888aaf65e214d2a8224134334be64fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3232726763.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model, args=args, train_dataset=tokenized, data_collator=collator, tokenizer=tok)\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magd9c\u001b[0m (\u001b[33mdhtynt-neer\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250918_212047-rw2gj5zv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dhtynt-neer/huggingface/runs/rw2gj5zv' target=\"_blank\">worthy-galaxy-9</a></strong> to <a href='https://wandb.ai/dhtynt-neer/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dhtynt-neer/huggingface' target=\"_blank\">https://wandb.ai/dhtynt-neer/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dhtynt-neer/huggingface/runs/rw2gj5zv' target=\"_blank\">https://wandb.ai/dhtynt-neer/huggingface/runs/rw2gj5zv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapter to lora_out\n"
     ]
    }
   ],
   "source": [
    "# Optional: LoRA fine-tune TinyLlama on the QA pairs (skip if you only want Gemini).\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch, json\n",
    "\n",
    "base_model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tok=AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "model=AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.float16 if torch.cuda.is_available() else None, device_map=\"auto\")\n",
    "\n",
    "qa=[json.loads(l) for l in open(\"qa_bank.jsonl\",\"r\",encoding=\"utf-8\")]\n",
    "def to_text(x): return f\"Question: {x['question']}\\nAnswer: {x['answer']}\"\n",
    "ds=Dataset.from_list([{\"text\": to_text(x)} for x in qa])\n",
    "\n",
    "def tokenize(batch): return tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "tokenized=ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "lora=LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=16, lora_dropout=0.05, target_modules=[\"q_proj\",\"v_proj\"])\n",
    "model=get_peft_model(model, lora)\n",
    "\n",
    "args=TrainingArguments(output_dir=\"lora_out\", num_train_epochs=1, per_device_train_batch_size=2, gradient_accumulation_steps=4,\n",
    "                       learning_rate=2e-4, logging_steps=5, save_steps=100, fp16=torch.cuda.is_available())\n",
    "collator=DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
    "\n",
    "trainer=Trainer(model=model, args=args, train_dataset=tokenized, data_collator=collator, tokenizer=tok)\n",
    "trainer.train()\n",
    "model.save_pretrained(\"lora_out\"); tok.save_pretrained(\"lora_out\")\n",
    "print(\"Saved LoRA adapter to lora_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knx-IFqvmmAB",
   "metadata": {
    "id": "knx-IFqvmmAB"
   },
   "source": [
    "## 9) Save Reproducibility Config ‚Üí rag_gemini_ft_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "iV5Tgr-dmmAB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iV5Tgr-dmmAB",
    "outputId": "85663916-29a7-411c-c6f2-b6bf593e6fb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"chunk_size\": 500,\n",
      "  \"chunk_overlap\": 100,\n",
      "  \"retriever_k\": 4,\n",
      "  \"embedding_model\": \"text-embedding-004\",\n",
      "  \"generation_model\": \"gemini-1.5-flash\",\n",
      "  \"chunk_settings\": [\n",
      "    {\n",
      "      \"size\": 500,\n",
      "      \"overlap\": 100\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuning\": {\n",
      "    \"A_simulated_memory\": true,\n",
      "    \"B_lora_peft\": \"optional\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "try:\n",
    "    cfg=json.load(open(\"rag_gemini_ft_config.json\"))\n",
    "except Exception: cfg={}\n",
    "cfg.update({\n",
    "    \"embedding_model\":\"text-embedding-004\",\n",
    "    \"generation_model\":\"gemini-1.5-flash\",\n",
    "    \"retriever_k\":4,\n",
    "    \"chunk_settings\":[{\"size\":500,\"overlap\":100}],\n",
    "    \"fine_tuning\":{\"A_simulated_memory\": True, \"B_lora_peft\":\"optional\"}\n",
    "})\n",
    "with open(\"rag_gemini_ft_config.json\",\"w\") as f: json.dump(cfg,f,indent=2)\n",
    "print(json.dumps(cfg, indent=2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
